\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[serbian]{babel}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{array}
\usepackage[a4paper]{geometry}
\usepackage{blindtext}
\usepackage{tocbibind} % Ubacuje literaturu u sadrzaj

\title{Key Features Net i njene primene}
\author{Milo\v s Vujasinovi\'c}
\date{Novi Sad, jul 2020}

\graphicspath{{img}}

\newtheorem{definition}{Definicija}


% tabularx columns
\newcolumntype{Y}{>{\centering\arraybackslash}X}

% Variables
\newcommand{\titlelogosize}{2.4cm}

\let\oldsection\section
\renewcommand\section{\clearpage\oldsection}

% custom make title
\makeatletter         
\renewcommand\maketitle{
	\thispagestyle{empty}
	\begin{tabularx}{\textwidth}{m{\titlelogosize}Ym{\titlelogosize}}	\includegraphics[width=\linewidth]{./pmflogo} &
		\begin{tabular}{@{}c@{}}
			УНИВЕРЗИТЕТ У НОВОМ САДУ\\
			ПРИРОДНО-МАТЕМАТИЧКИ ФАКУЛТЕТ\\
			ДЕПАРТМАН ЗА МАТЕМАТИКУ И\\
			ИНФОРМАТИКУ
		\end{tabular} &
		\includegraphics[width=\linewidth]{./unslogo}
	\end{tabularx}
	
	\vfill
	
	\begin{center}
		\LARGE
		\textbf{\@title}
		
		\medskip
		
		Seminarski rad
	\end{center}
	
	\vfill
	
	\begin{flushright}
		\large
		\textbf{\@author}
	\end{flushright}
	
	\bigskip
	\bigskip
	
	\begin{center}
		\large
		\@date
	\end{center}
	\pagebreak
	
	\clearpage
	\pagenumbering{arabic} 
}
\makeatother

\begin{document}
	\maketitle
	
	\tableofcontents
	
	\section*{Uvod}
	\addcontentsline{toc}{section}{\protect{}Uvod}
	
	Autoenkoderi su ve\' c godinama zlatni standard u smanjenju dimenzionalnosti podataka. Na\v cin na koji rade se pokazao kao veoma efikasan u otklanjanju \v suma i dopunjavanju podataka koji su o\v ste\' ceni. Motivisan datim primerima, ovaj rad poku\v sava da prika\v ze novi na\v cin gledanja na smanjene dimenzionalnosti podataka ma\v sinskim u\v cenjem. Zatim, primenom iznesenih ideja i nekih od principa koji se nalaze u osnovi autoenkodera se uvodi model neuronske mre\v ze koji za cilj da iz podataka koji se prosle\dj uju modelu izdvoji najbitnije odlike za klasifikaciju. Kroz ovaj postupak se tako\dj e razmatraju novi na\v cini treniranja i evaluacije modela, a na kraju se rezultati datog modela porede sa rezultatima tradicionalnih autoenkodera.
	
	\section{Poglavlje 1}
	
	Autoenkoderi su neuronske mre\v ze koje se treniraju da opona\v saju identi\v cko preslikavanje, to jest da ulaz preslikava na samog sebe. 
	Ovo samo po sebi neizgleda veoma korisno, me\dj utim autoenkoderi, pored navedenih, sadr\v ze i osobinu da je broj neurona u najmanjem skrivenom sloju mre\v ze manji od broja neurona ulaznog i izlaznog sloja. 
	Ovo za implikaciju ima da se prilikom prolaska podataka kroz mre\v zu u datom sloju nalazi reprezentacija podataka za \v cije je predstavljanje potreban manji broj memorijskih jedinica od originalnih podataka i ovo mo\v ze da varira od nekoliko jedinica, pa do redova veli\v cina manje. 
	Dati skriveni sloj se naziva \emph{usko grlo} (engl. \emph{bottle neck}) i on deli neuronsku mre\v zu na dva dela: od ulaza do sebe i od sebe do izlaza. 
	Ovi delovi se redom nazivaju \emph{enkoder} i \emph{dekoder}, a oni se koriste za preslikavanje originalnih podataka u reprezentaciju smanjenih dimenzija i nazad.
	
	Prilikom treniranja autoenkodera se gre\v ska ra\v cuna kao gre\v ska izme\dj u svakog neurona ulaza i njemu odgovaraju\'ceg neurona izlaza. 
	Ovo je veoma intuitivno re\v senje ako \v zelimo da rezultati na izlazu budu naizgled \v sto sli\v cniji ulaznim podacima. 
	Me\dj utim dato preslikavanje je u praksi gotovo nemogu\'ce napraviti tako da bude bude savr\v seno.
	Kao posledica dolazi do izvesne gre\v ske izme\dj u originalnih podataka i rezultata mre\v ze koji mo\v zda neizgledaju zna\v cajno ili su nam gotovo neprimetni, ali, teoretski, mogu zna\v cajno da uti\v cu na rezultate klasifikacije ako bi se rezultati autoenkodera pustili kroz klasifikator i uporedili sa rezultatima klasifikacije originalnih podataka.
	Na primer, ako imamo klasifikator koji traga za odlikom koja je sadr\v zana u veoma malom broju memorijskih jedinica u pore\dj enju sa veli\v cinom jedne instance podataka, a autonkoder zaklju\v ci da bi uvr\v stavanje date odlike samo pove\'calo gre\v sku, jer bi njenim uvr\v stavanjem bilo onemogu\'ceno uvr\v stavanje neke druge odlike koja je iz pogleda gre\v ske autoenkodera va\v znija.
	Kao posledica, redukcija podataka ovim postupkom je ne potencijalno samo beskorisna, nego i \v cini podatke beskorisnim ako \v zelimo da ih klasifikujemo ili, jo\v s gore, koristimo za treniranje klasifikatora.
	
	Diskutovani problem svakako postavlja pitanje kako se mo\v ze prevazi\'ci, ali pre davanja odgovora na njega, potrobno je da defini\v semo podelu koja \'ce da omogu\'ci uo\v cavanje ovog problema jasnijim i lak\v sim.
	
	\begin{definition}
		Autoenkoder koji za \v cije se treniranje koristi znanje o samim podacima se naziva \emph{izgledni autoenkoder}, dok autoenkoder za \v cije se treniranje koristi znanje o klasama kojima podaci pripadaju se naziva \emph{su\v stinski autoenkoder}. Treba imati u vidu da date osobine nisu disjunktne, pa postoji i \emph{izgledno-su\v stinski autoenkoder}.
	\end{definition}
	
	
	
	\section*{Zaklju\v cak}
	\addcontentsline{toc}{section}{\protect{}Zaklju\v cak}
	
	\pagebreak
	\bibliographystyle{unsrt}
	\bibliography{references}
\end{document}