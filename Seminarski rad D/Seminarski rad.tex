\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[serbian]{babel}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{array}
\usepackage[a4paper]{geometry}
\usepackage{blindtext}
\usepackage{tocbibind} % Ubacuje literaturu u sadrzaj
\usepackage{amsmath}
\usepackage{amssymb}


\title{Key Features Net i njene primene}
\author{Milo\v s Vujasinovi\'c}
\date{Novi Sad, jul 2020}

\graphicspath{{img}}

\newtheorem{definition}{Definicija}


% tabularx columns
\newcolumntype{Y}{>{\centering\arraybackslash}X}

% Variables
\newcommand{\titlelogosize}{2.4cm}

% Adding pagebreak before section
\let\oldsection\section
\renewcommand\section{\clearpage\oldsection}

% custom make title
\makeatletter         
\renewcommand\maketitle{
	\thispagestyle{empty}
	\begin{tabularx}{\textwidth}{m{\titlelogosize}Ym{\titlelogosize}}	\includegraphics[width=\linewidth]{./pmflogo} &
		\begin{tabular}{@{}c@{}}
			УНИВЕРЗИТЕТ У НОВОМ САДУ\\
			ПРИРОДНО-МАТЕМАТИЧКИ ФАКУЛТЕТ\\
			ДЕПАРТМАН ЗА МАТЕМАТИКУ И\\
			ИНФОРМАТИКУ
		\end{tabular} &
		\includegraphics[width=\linewidth]{./unslogo}
	\end{tabularx}
	
	\vfill
	
	\begin{center}
		\LARGE
		\textbf{\@title}
		
		\medskip
		
		Seminarski rad
	\end{center}
	
	\vfill
	
	\begin{flushright}
		\large
		\textbf{\@author}
	\end{flushright}
	
	\bigskip
	\bigskip
	
	\begin{center}
		\large
		\@date
	\end{center}
	\pagebreak
	
	\clearpage
	\pagenumbering{arabic} 
}
\makeatother

\begin{document}
	\maketitle
	
	\tableofcontents
	
	\section*{Uvod}
	\addcontentsline{toc}{section}{\protect{}Uvod}
	
	Autoenkoderi su ve\' c godinama zlatni standard u smanjenju dimenzionalnosti podataka. Na\v cin na koji rade se pokazao kao veoma efikasan u otklanjanju \v suma i dopunjavanju podataka koji su o\v ste\' ceni. Motivisan datim primerima, ovaj rad poku\v sava da prika\v ze novi na\v cin gledanja na smanjene dimenzionalnosti podataka ma\v sinskim u\v cenjem. Zatim, primenom iznesenih ideja i nekih od principa koji se nalaze u osnovi autoenkodera se uvodi model neuronske mre\v ze koji za cilj da iz podataka koji se prosle\dj uju modelu izdvoji najbitnije odlike za klasifikaciju. Kroz ovaj postupak se tako\dj e razmatraju novi na\v cini treniranja i evaluacije modela, a na kraju se rezultati datog modela porede sa rezultatima tradicionalnih autoenkodera.
	
	\section{Poglavlje 1}
	
	\subsection{Su\v stina podataka i problem sa tradicionalnim autoenkoderom}
	
	Autoenkoderi su neuronske mre\v ze koje se treniraju da opona\v saju identi\v cko preslikavanje, to jest da ulaz preslikava na samog sebe. 
	Ovo samo po sebi ne izgleda veoma korisno, me\dj utim autoenkoderi, pored navedenih, sadr\v ze i osobinu da je broj neurona u najmanjem skrivenom sloju mre\v ze manji od broja neurona ulaznog i izlaznog sloja. 
	Ovo za implikaciju ima da se prilikom prolaska podataka kroz mre\v zu u datom sloju nalazi reprezentacija podataka za \v cije je predstavljanje potreban manji broj memorijskih jedinica od originalnih podataka i ovo mo\v ze da varira od nekoliko jedinica, pa do redova veli\v cina manje. 
	Dati skriveni sloj se naziva \emph{usko grlo} (engl. \emph{bottle neck}) i on deli neuronsku mre\v zu na dva dela: od ulaza do sebe i od sebe do izlaza. 
	Ovi delovi se redom nazivaju \emph{enkoder} i \emph{dekoder}, a oni se koriste za preslikavanje originalnih podataka u reprezentaciju smanjenih dimenzija i nazad. U nastavku rada \'ce se autoenkoderi definisati na formalniji na\v cin, ali za trenutne potrebe dato obja\v snjenje je dovoljno.
	
	Prilikom treniranja autoenkodera se gre\v ska ra\v cuna kao gre\v ska izme\dj u svakog neurona ulaza i njemu odgovaraju\'ceg neurona izlaza. 
	Ovo je veoma intuitivno re\v senje ako \v zelimo da rezultati na izlazu budu naizgled \v sto sli\v cniji ulaznim podacima. 
	Me\dj utim dato preslikavanje je u praksi gotovo nemogu\'ce napraviti tako da bude bude savr\v seno.
	Kao posledica dolazi do izvesne gre\v ske izme\dj u originalnih podataka i rezultata mre\v ze koji mo\v zda neizgledaju zna\v cajno ili su nam gotovo neprimetni, ali, teoretski, mogu zna\v cajno da uti\v cu na rezultate klasifikacije ako bi se rezultati autoenkodera pustili kroz klasifikator i uporedili sa rezultatima klasifikacije originalnih podataka.
	Na primer, ako imamo klasifikator koji traga za odlikom koja je sadr\v zana u veoma malom broju memorijskih jedinica u pore\dj enju sa veli\v cinom jedne instance podataka, a autonkoder zaklju\v ci da bi uvr\v stavanje date odlike samo pove\'calo gre\v sku, jer bi njenim uvr\v stavanjem bilo onemogu\'ceno uvr\v stavanje neke druge odlike koja je iz pogleda gre\v ske autoenkodera va\v znija.
	Kao posledica, redukcija podataka ovim postupkom je ne potencijalno samo beskorisna, nego i \v cini podatke beskorisnim ako \v zelimo da ih klasifikujemo ili, jo\v s gore, koristimo za treniranje klasifikatora.
	
	Diskutovani problem svakako postavlja pitanje kako se mo\v ze prevazi\'ci, ali pre davanja odgovora na njega, potrobno je da defini\v semo podelu koja \'ce da omogu\'ci uo\v cavanje ovog problema jasnijim i lak\v sim. 
	Iz prikazanog rezonovanja se jasno mo\v ze uvideti da se autoenkoderi mogu trenirati ili u svrhu o\v cuvanja izgleda podataka ili u svrhu o\v cuvanja onoga \v sto podaci ustvari jesu, odnosno njihove \emph{su\v stine}.
	Treba imati u vidu da podaci imaju gotovo bezbroj razli\v citih su\v stina u zavisnosti od toga iz kog se ugla posmatraju, to jest da se u zavisnosti od pitanja koje se postavlja o njima njihova su\v stina menja i odatle mo\v zemo da spojimo ideju o su\v stini sa problemom klasifikacije tako \v sto \'cemo re\'ci da pitanje koje postavlja klasifikacioni problem odre\dj uje su\v stinu podataka o kojoj u datom trenutku pri\v camo.
	Ove ideje se koriste za podelu definisanu u definiciji \ref{autoencoder-training-based-types}. Iz ove definicije je jasno da su tradicionalni autoenkoderi ustvari izgledni autoenkoderi, dok autoenkoderi kojima se te\v zi u svrhu prevazila\v zenja diskutovanog problema su su\v stinski autoenkoderi, mada se i za njih, u zavisnosti od implementacije, postavlja pitanje koliko dobro mogu da o\v cuvaju odlike bitne za klasifikaciju.
	
	\begin{definition}[Podela autoenkodera po na\v cinu treniranja]
		\label{autoencoder-training-based-types}
		Autoenkoder za \v cije se treniranje koristi znanje o samim podacima se naziva \emph{izgledni autoenkoder}, dok autoenkoder za \v cije se treniranje koristi znanje o klasama kojima podaci pripadaju se naziva \emph{su\v stinski autoenkoder}. Treba imati u vidu da date osobine nisu disjunktne, pa postoji i \emph{izgledno-su\v stinski autoenkoder}.
	\end{definition}


	\subsection{Podpoglavlje 2}
	
	Da bismo razmatrali kako se klase kojima podaci pripadaju mogu uklju\v citi u treniranje autoenkodera defini\v simo prvo autoenkodere na formalniji na\v cin. 
	Za ovo \'cemo iskoristiti definiciju \ref{general-autoencoder-framework} preuzetu iz \cite[Poglavlje 2]{pmlr-v27-baldi12a}.
	
	\begin{definition}[Op\v sti autoenkoder \textit{framework} \cite{pmlr-v27-baldi12a}]
		\label{general-autoencoder-framework}
		\sloppy $n/p/n$ autoenkoder je definisan kao $t$-torka $n, p, m, \mathbb{F}, \mathbb{G}, \mathcal{A}, \mathcal{B}, \mathcal{X}, \Delta$ gde va\v zi:
		\begin{enumerate}
			\addtolength{\itemindent}{1em}
			\item $\mathbb{F}$ i $\mathbb{G}$ su skupovi.
			\item $n$ i $p$ su pozitivni celi brojevi. (Autor razmatra slu\v caj kada je $0 < p < n$ \v sto se podrazumeva prilikom pravljenja autoenkodera)
			\item $\mathcal{A}: \mathbb{G}^p \to \mathbb{F}^n$
			\item $\mathcal{B}: \mathbb{F}^n \to \mathbb{G}^p$
			\item $\mathcal{X} = \{x_1, \ldots, x_m\}$ je skup od $m$ (trening) vektora u $\mathbb{F}^n$. Ako se radi o spolja\v snjem skupu ciljeva u $\mathbb{F}^n$ ozna\v cavamo ga sa $\mathcal{Y} = \{y_1, \ldots, y_n\}$.
			\item $\Delta$ je funkcija razli\v citosti ili distorcije definisana nad $\mathbb{F}^n$
		\end{enumerate}
	\end{definition}
	
	Da bismo bolje razumeli datu definiciju i bli\v ze se upoznali sa terminologijom koja \'ce se koristiti u nastavku pro\dj imo kroz sve elemente date $t$-torke koja se koristi za predstavljanje autoenkodera. 
	Kao \v sto smo ranije pri\v cali ideja iza autoenkodera je da nau\v ci da slika ulaz na samog sebe, a da pri tome postoji skriveni sloj koji je manjih dimenzija od samog ulaza i na ovaj na\v cin se posti\v ze smanjenje dimenzionalnosti podataka.
	$n$ je ni\v sta manje nego broj dimenzija ulaza kao i izlaza, dok je $p$ broj dimenzija \emph{uskog grla}\footnote{sloj u kome se prilikom prolaza podataka kroz autoenkoder nalazi reprezentacija podataka smanjenih dimenzija}, po\v sto se cilja da je broj dimenzija uskog grla manji i broja dimenzija ulaza zato se i pretpostavlja da je $0 < p < n$. 
	Svaka memorijska jedinica u ulaznom i izlaznom sloju, odnosno uskog grlu sadr\v zi podatke koji redom pripadaju skupovima $\mathbb{F}$ i $\mathbb{G}$. $\mathcal{B}$ je funkcija koja preslikava ulazne podatke u reprezentaciju smanjenih dimenzija i ona se naziva \emph{enkoder}. $\mathcal{A}$ radi obrnut proces od $\mathcal{B}$ i preslikava reprezentaciju smanjenih dimenzija na izlaz i ova funkcija se naziva \emph{dekoder}.
	Za treniranje autoenkodera su potrebni podaci i skup trening podataka je u datoj definiciji ozna\v cen sa $\mathcal{X}$.
	Me\dj utim, podaci se ne koriste samo za treniranje i mo\v zemo imati podatke koji se koriste za evaluaciju ili podatke koji se pu\v staju kroz autoenkoder u cilju smanjenja dimenzija, ovakvi skupovi podataka su u definiciji ozna\v ceni sa $\mathcal{Y}$.
	
	
	\section*{Zaklju\v cak}
	\addcontentsline{toc}{section}{\protect{}Zaklju\v cak}
	
	\pagebreak
	\bibliographystyle{unsrt}
	\bibliography{references}
\end{document}