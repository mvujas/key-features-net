\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[serbian]{babel}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{array}
\usepackage[a4paper]{geometry}
\usepackage{blindtext}
\usepackage{tocbibind} % Ubacuje literaturu u sadrzaj
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usetikzlibrary{positioning,calc}
\tikzset{%
	every neuron/.style={
		circle,
		draw,
		minimum size=1cm
	},
	neuron missing/.style={
		draw=none, 
		scale=4,
		text height=0.333cm,
		execute at begin node=\color{black}$\vdots$
	},
}


% makes TOC clickable
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

\title{Key Features Net i njene primene}
\author{Milo\v s Vujasinovi\'c}
\date{Novi Sad, jul 2020}

\graphicspath{{img}}

\newtheorem{definition}{Definicija}


% tabularx columns
\newcolumntype{Y}{>{\centering\arraybackslash}X}

% Variables
\newcommand{\titlelogosize}{2.4cm}

% Adding pagebreak before section
\let\oldsection\section
\renewcommand\section{\clearpage\oldsection}

% custom make title
\makeatletter         
\renewcommand\maketitle{
	\thispagestyle{empty}
	\begin{tabularx}{\textwidth}{m{\titlelogosize}Ym{\titlelogosize}}	\includegraphics[width=\linewidth]{./pmflogo} &
		\begin{tabular}{@{}c@{}}
			УНИВЕРЗИТЕТ У НОВОМ САДУ\\
			ПРИРОДНО-МАТЕМАТИЧКИ ФАКУЛТЕТ\\
			ДЕПАРТМАН ЗА МАТЕМАТИКУ И\\
			ИНФОРМАТИКУ
		\end{tabular} &
		\includegraphics[width=\linewidth]{./unslogo}
	\end{tabularx}
	
	\vfill
	
	\begin{center}
		\LARGE
		\textbf{\@title}
		
		\medskip
		
		Seminarski rad
	\end{center}
	
	\vfill
	
	\begin{flushright}
		\large
		\textbf{\@author}
	\end{flushright}
	
	\bigskip
	\bigskip
	
	\begin{center}
		\large
		\@date
	\end{center}
	\pagebreak
	
	\clearpage
	\pagenumbering{arabic} 
}
\makeatother

\begin{document}
	\maketitle
	
	\tableofcontents
	
	\section*{Uvod}
	\addcontentsline{toc}{section}{\protect{}Uvod}
	
	Autoenkoderi su ve\' c godinama zlatni standard u smanjenju dimenzionalnosti podataka. Na\v cin na koji rade se pokazao kao veoma efikasan u otklanjanju \v suma i dopunjavanju podataka koji su o\v ste\' ceni. Motivisan datim primerima, ovaj rad poku\v sava da prika\v ze novi na\v cin gledanja na smanjene dimenzionalnosti podataka ma\v sinskim u\v cenjem. Zatim, primenom iznesenih ideja i nekih od principa koji se nalaze u osnovi autoenkodera se uvodi model neuronske mre\v ze koji ima za cilj da iz podataka koji se prosle\dj uju modelu izdvoji najbitnije odlike za klasifikaciju. Kroz ovaj postupak se tako\dj e razmatraju novi na\v cini treniranja i evaluacije modela, a na kraju se rezultati datog modela porede sa rezultatima tradicionalnih autoenkodera.
	
	\section{Su\v stinski autoenkoderi}
	
	\subsection{Su\v stina podataka i problem sa tradicionalnim autoenkoderom}
	
	Autoenkoderi su neuronske mre\v ze koje se treniraju da opona\v saju identi\v cko preslikavanje, to jest da ulaz preslikava na samog sebe. 
	Ovo samo po sebi ne izgleda veoma korisno, me\dj utim autoenkoderi, pored navedenih, sadr\v ze i osobinu da je broj neurona u najmanjem skrivenom sloju mre\v ze manji od broja neurona ulaznog i izlaznog sloja. 
	Ovo za implikaciju ima da se prilikom prolaska podataka kroz mre\v zu u datom sloju nalazi reprezentacija podataka za \v cije je predstavljanje potreban manji broj memorijskih jedinica od originalnih podataka i ovo mo\v ze da varira od nekoliko jedinica, pa do redova veli\v cina manje. 
	Dati skriveni sloj se naziva \emph{usko grlo} (engl. \emph{bottle neck}) i on deli neuronsku mre\v zu na dva dela: od ulaza do sebe i od sebe do izlaza. 
	Ovi delovi se redom nazivaju \emph{enkoder} i \emph{dekoder}, a oni se koriste za preslikavanje originalnih podataka u reprezentaciju smanjenih dimenzija i nazad. U nastavku rada \'ce se autoenkoderi definisati na formalniji na\v cin, ali za trenutne potrebe dato obja\v snjenje je dovoljno.
	
	Prilikom treniranja autoenkodera se gre\v ska ra\v cuna kao gre\v ska izme\dj u svakog neurona ulaza i njemu odgovaraju\'ceg neurona izlaza. 
	Ovo je veoma intuitivno re\v senje ako \v zelimo da rezultati na izlazu budu naizgled \v sto sli\v cniji ulaznim podacima. 
	Me\dj utim dato preslikavanje je u praksi gotovo nemogu\'ce napraviti tako da bude bude savr\v seno.
	Kao posledica dolazi do izvesne gre\v ske izme\dj u originalnih podataka i rezultata mre\v ze koji mo\v zda ne izgledaju zna\v cajno ili su nam gotovo neprimetni, ali, teoretski, mogu zna\v cajno da uti\v cu na rezultate klasifikacije ako bi se rezultati autoenkodera pustili kroz klasifikator i uporedili sa rezultatima klasifikacije originalnih podataka.
	Na primer, ako imamo klasifikator koji traga za odlikom koja je sadr\v zana u veoma malom broju memorijskih jedinica u pore\dj enju sa veli\v cinom jedne instance podataka, a autonkoder zaklju\v ci da bi uvr\v stavanje date odlike samo pove\'calo gre\v sku, jer bi njenim uvr\v stavanjem bilo onemogu\'ceno uvr\v stavanje neke druge odlike koja je iz pogleda gre\v ske autoenkodera va\v znija.
	Kao posledica, redukcija podataka ovim postupkom je ne potencijalno samo beskorisna, nego i \v cini podatke neupotrebljivim ako \v zelimo da ih klasifikujemo ili, jo\v s gore, koristimo za treniranje klasifikatora.
	
	Diskutovani problem svakako postavlja pitanje kako se mo\v ze prevazi\'ci, ali pre davanja odgovora na njega, potrobno je da defini\v semo podelu koja \'ce da omogu\'ci uo\v cavanje ovog problema jasnijim i lak\v sim. 
	Iz prikazanog rezonovanja se jasno mo\v ze uvideti da se autoenkoderi mogu trenirati ili u svrhu o\v cuvanja izgleda podataka ili u svrhu o\v cuvanja onoga \v sto podaci ustvari jesu, odnosno njihove \emph{su\v stine}.
	Treba imati u vidu da podaci imaju gotovo bezbroj razli\v citih su\v stina u zavisnosti od toga iz kog se ugla posmatraju, to jest da se u zavisnosti od pitanja koje se postavlja o njima njihova su\v stina menja i odatle mo\v zemo da spojimo ideju o su\v stini sa problemom klasifikacije tako \v sto \'cemo re\'ci da pitanje koje postavlja klasifikacioni problem odre\dj uje su\v stinu podataka o kojoj u datom trenutku pri\v camo.
	Ove ideje se koriste za podelu definisanu u definiciji \ref{autoencoder-training-based-types}. Iz ove definicije je jasno da su tradicionalni autoenkoderi ustvari izgledni autoenkoderi, dok autoenkoderi kojima se te\v zi u svrhu prevazila\v zenja diskutovanog problema su su\v stinski autoenkoderi, mada se i za njih, u zavisnosti od implementacije, postavlja pitanje koliko dobro mogu da o\v cuvaju odlike bitne za klasifikaciju.
	
	\begin{definition}[Podela autoenkodera po na\v cinu treniranja]
		\label{autoencoder-training-based-types}
		Autoenkoder za \v cije se treniranje koristi znanje o samim podacima se naziva \emph{izgledni autoenkoder}, dok autoenkoder za \v cije se treniranje koristi znanje o klasama kojima podaci pripadaju se naziva \emph{su\v stinski autoenkoder}. Treba imati u vidu da date osobine nisu me\dj usobno disjunktne, pa postoji i \emph{izgledno-su\v stinski autoenkoder}.
	\end{definition}


	\subsection{Treniranje su\v stinskih autoenkodera}
	
	Da bismo razmatrali kako se klase kojima podaci pripadaju mogu uklju\v citi u treniranje autoenkodera defini\v simo prvo autoenkodere na formalniji na\v cin. 
	Za ovo \'cemo iskoristiti definiciju \ref{general-autoencoder-framework} preuzetu iz \cite[Poglavlje 2]{pmlr-v27-baldi12a}.
	
	\begin{definition}[Op\v sti autoenkoder \textit{framework} \cite{pmlr-v27-baldi12a}]
		\label{general-autoencoder-framework}
		\sloppy $n/p/n$ autoenkoder je definisan kao $t$-torka $n, p, m, \mathbb{F}, \mathbb{G}, \mathcal{A}, \mathcal{B}, \mathcal{X}, \Delta$ gde va\v zi:
		\begin{enumerate}
			\addtolength{\itemindent}{1em}
			\item $\mathbb{F}$ i $\mathbb{G}$ su skupovi.
			\item $n$ i $p$ su pozitivni celi brojevi. (Autor razmatra slu\v caj kada je $0 < p < n$ \v sto se podrazumeva prilikom pravljenja autoenkodera)
			\item $\mathcal{A}$ je klasa funkcija koje preslikavaju ulaz iz  $\mathbb{G}^p$ na $\mathbb{F}^n$
			\item $\mathcal{B}$ je klasa funkcija koje preslikavaju ulaz iz $\mathbb{F}^n$ na $\mathbb{G}^p$
			\item $\mathcal{X} = \{x_1, \ldots, x_m\}$ je skup od $m$ (trening) vektora u $\mathbb{F}^n$. Ako se radi o spolja\v snjem skupu ciljeva u $\mathbb{F}^n$ ozna\v cavamo ga sa $\mathcal{Y} = \{y_1, \ldots, y_n\}$.
			\item $\Delta$ je funkcija razli\v citosti ili distorcije definisana nad $\mathbb{F}^n$
		\end{enumerate}
	\end{definition}
	
	Da bismo bolje razumeli datu definiciju i bli\v ze se upoznali sa terminologijom koja \'ce se koristiti u nastavku pro\dj imo kroz sve elemente date $t$-torke koja se koristi za predstavljanje autoenkodera. 
	Kao \v sto smo ranije pri\v cali ideja iza autoenkodera je da nau\v ci da slika ulaz na samog sebe, a da pri tome postoji skriveni sloj koji je manjih dimenzija od samog ulaza i na ovaj na\v cin se posti\v ze smanjenje dimenzionalnosti podataka.
	$n$ je ni\v sta manje nego broj dimenzija ulaza kao i izlaza, dok je $p$ broj dimenzija \emph{uskog grla}\footnote{sloj u kome se prilikom prolaza podataka kroz autoenkoder nalazi reprezentacija podataka smanjenih dimenzija}, po\v sto se cilja da je broj dimenzija uskog grla manji od broja dimenzija ulaza zato se i pretpostavlja da je $0 < p < n$. 
	Svaka memorijska jedinica u ulaznom i izlaznom sloju, odnosno uskog grlu sadr\v zi podatke koji redom pripadaju skupovima $\mathbb{F}$ i $\mathbb{G}$. $\forall B \in \mathcal{B}$ je funkcija koja preslikava ulazne podatke u reprezentaciju smanjenih dimenzija i ona se naziva \emph{enkoder}. $\forall A \in \mathcal{A}$ radi obrnut proces od funkcija iz $\mathcal{B}$ i preslikava reprezentaciju smanjenih dimenzija na izlaz i ova funkcija se naziva \emph{dekoder}.
	Treba primetiti da se ovde delovi neuronske mre\v ze posmatraju kao funkcije, jer oni to i jesu, i ova notacija \'ce se u nastavku koristiti kada se govori o neuronskim mre\v zama i njenim delovima.
	Za treniranje autoenkodera su potrebni podaci i skup trening podataka je u datoj definiciji ozna\v cen sa $\mathcal{X}$.
	Me\dj utim, podaci se ne koriste samo za treniranje i mo\v zemo imati podatke koji se koriste za evaluaciju ili podatke koji se pu\v staju kroz autoenkoder u cilju smanjenja dimenzionalnosti. 
	Ovakvi skupovi podataka su u definiciji ozna\v ceni sa $\mathcal{Y}$. Na kraju, ostala je jo\v s da se objasni funkcija razli\v citosti ili distorcije $\Delta$ koja igra va\v znu ulogu u pprocesu treniranja autoenkodera.
	Cilj treninga autoenkodera je pronalazak funkcija $A \in \mathcal{A}$ i $B \in \mathcal{B}$ za koje je vrednost funkcije distorcije minimalna nad celim skupom trening vektora $\mathcal{X}$. 
	Funkcija distorcije $\Delta$ je primer onoga \v sto kod neuronskih mre\v za znamo kao funkcija gre\v ske ili \emph{loss} funkcija.
	
	Funkcija gre\v ske je jedna od glavnih karakteristika treninga neuronskih mre\v za i ona govori u kom smeru \'ce se treniranje kretati. 
	Kao takva ona je o\v cigledna opcija za uklju\v cenje klasa kojima podaci pripadaju u trening i na taj na\v cin dobijanje su\v stinskog autoenkodera. 
	Me\dj utim, ovo nije tako lako izvesti kao \v sto zvu\v ci. 
	Dok se kod tradicionalnih, izglednih, autoenkodera srednja kvadratna gre\v ska nudi kao idealna funkcija gre\v ske, kod su\v stinskih autoenkodera stvari su malo komplikovanije i zahtevaju se odre\dj eni trikovi u cilju dobijanja \v zeljenog efekta.
	Glavni problem je \v sto kod treniranja izglednih autoenkodera je vrednost koju mre\v za treba da proizvede poznata, dok kod su\v stinskih autoenkodera je poznata samo vrednost koja treba da se dobije kada se tra\v zeni rezultati puste kroz odre\dj enu funkciju.
	Na osnovu ovoga definicija \ref{types-of-machine-learning-training} uvodi dva na\v cina treniranja na koja se model ma\v sinskog u\v cenja mo\v ze u\v citi.
	
	\begin{definition}[Podela na\v cina treniranja modela ma\v sinskog u\v cenja]
		\label{types-of-machine-learning-training}
		Ako je prilikom treniranja modela ma\v sinskog u\v cenja poznata vrednost koja se treba dobiti kao proizvod rada modela onda se dati proces treniranja naziva \emph{direktno treniranje}. 
		Nasuprot tome, treniranje u kome je poznata samo vrednost koja se treba dobiti nakon \v sto se rezultati rada modela puste kroz odre\dj enu funkciju se naziva \emph{indirektno treniranje}.
	\end{definition}

	\begin{figure}[h!]
		\centering
		\begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
		
		\foreach \m/\l [count=\y] in {1,2,3,missing,4}
		\node [every neuron/.try, neuron \m/.try] (input-autoencoder-\m) at (0,2.5-\y) {};
		
		\foreach \m [count=\y] in {1,2,3,missing,4}
		\node [every neuron/.try, neuron \m/.try ] (output-autoencoder-\m) at (2,2.5-\y) {};
		
		\foreach \m [count=\y] in {1,2,3,missing,4}
		\node [every neuron/.try, neuron \m/.try ] (input-classificator-\m) at (4.5,2.5-\y) {};
		
		\foreach \m [count=\y] in {1,missing,2}
		\node [every neuron/.try, neuron \m/.try ] (output-classificator-\m) at (6.5,1.5-\y) {};
		
		\foreach \l [count=\i] in {1,2,3,n}
		\draw [<-] (input-autoencoder-\i) -- ++(-1,0)
		node [above, midway] {$I_\l$};
		
		\foreach \l [count=\i] in {1,2,3,n}
		\draw [<-] (input-classificator-\i) -- (output-autoencoder-\i)
		node [above, midway] {$I'_\l$};
		
		\foreach \l [count=\i] in {1,m}
		\draw [->] (output-classificator-\i) -- ++(1,0)
		node [above, midway] {$\sigma_\l$};
		
		\foreach \i in {1,...,4}
		\foreach \j in {1,...,4}
		\draw [->] (input-autoencoder-\i) -- (output-autoencoder-\j);
		
		\foreach \i in {1,...,4}
		\foreach \j in {1,...,2}
		\draw [->] (input-classificator-\i) -- (output-classificator-\j);
		
		\node [align=center, above] at (0,2) {Originalni \\podaci};
		\node [align=center, above] at (2,2) {Rezultat \\autoenkodera};
		\node [align=center, above] at (4.5,2) {Ulaz \\klasifikatora};
		\node [align=center, above] at (6.5,2) {Distribucija \\verovatno\'ce};
		
		\coordinate (autoencoder-mid) at ($(input-autoencoder-3)!.5!(output-autoencoder-3)$);
		\coordinate (classificator-mid) at ($(input-classificator-3)!.5!(output-classificator-1)$);
		
		\node[fill=white,scale=3,inner xsep=1pt,inner ysep=10mm] at (autoencoder-mid)
		{$\dots$};
		
		\node[fill=white,scale=3,inner xsep=1pt,inner ysep=10mm] at (autoencoder-mid -| classificator-mid) {$\dots$};
		
		\draw [
			thick,
			decoration={
				brace,
				mirror,
				raise=.7cm
			},
			decorate
		] (input-autoencoder-4.west) -- (output-autoencoder-4.east) 
		node [pos=0.5,anchor=north,yshift=-0.75cm] {Autoenkoder}; 
		
		\draw [
			thick,
			decoration={
				brace,
				mirror,
				raise=.7cm
			},
			decorate
		]  (input-classificator-4.west) -- (output-classificator-2.east |- input-classificator-4.west)
		node [pos=0.5,anchor=north,yshift=-0.75cm] {Klasifikator}; 
		
		\end{tikzpicture}
		\caption{Model klasifikatora dobijen spajanjem autoenkodera i manjeg klasifikatora}
		\label{autoencoder-classifier-model-combination}
	\end{figure}

	Iako su su\v stinski autoenkoderi dati kao primer indirektnog treniranja, uz pomo\'c malog trika, se mo\v ze napraviti da njihovo treniranje bude deo direktnog procesa treniranja.
	Po\v sto znamo da rezultat su\v stinskog autoenkodera kada se pusti kroz klasifikator treba da da nama poznatu klasu, mo\v zemo ovo svesti na istovremeno treniranje autoenkodera i klasifikatora.
	Ideja je da imamo strukturu autoenkodera i klasifikatora i da od njih napravimo novu neuronsku mre\v zu tako \v sto \'cemo staviti da ulaz prvo prolazi kroz autoenkoder, potom da izlaz autoenkodera bude ulaz u dati klasifikator, a sve zajedno treniramo kao da je jedan veliki klasifikator.
	Po\v sto znamo da je izlaz autoenkodera istih dimenzija kao i ulaz, koji je ujedno istih dimenzija kao i ulaz klasifikatora, nema problema prilikom prosle\dj ivanja rezultata autoenkodera klasifikatoru, jer su dimenzije odgovaraju\'cih slojeva jednake.
	Grafik na kojem je prikazan jedan ovakav model se mo\v ze videti na slici \ref{autoencoder-classifier-model-combination}.
	Ova ideja iako zvu\v ci mogu\'ce ipak sa sobom nosi dva problema.
	Prvi je \v sto imamo veliki model \v sto za posledicu ima du\v ze vreme treniranja i mo\v ze da na\v skodi kvalitetu istreniranog modela.
	Drugi problem je ono \v sto \v cini dati pristup beskorisnim, ali da bismo njega shvatili moramo se vratiti na koncept su\v stine podataka i njene korelacije sa originalnim podacima.
	Su\v stina podataka je rezultat razmi\v sljanja kako iz podataka mo\v ze da se ukloni \v sto je vi\v se mogu\'ce osobina tako da rezultat klasifikacije podataka koji sadr\v ze preostale osobine bude isti kao rezultat klasifikacije originalnih podataka.
	Ovo bi zna\v cilo da ako imamo savr\v sen su\v stinski autoenkoder i znamo da se ulazni podaci klasifikakuju u odre\dj enu klasu da bi onda i rezultat autoenkodera za date podatke morao da se klasifikuje u istu klasu, kao i obrnuto.
	Rezultat prethodno opisanog procesa treniranja bi bili autoenkoder i klasifikator i iako bi rezultat klasifikacije kroz dati klasifikator podataka propu\v stenih kroz dati autoenkoder te\v zio ka klasi ka kojoj je treniran, u datom procesu treniranja ne postoji mehanizam koji obezbe\dj uje da \'ce originalni podaci biti dobro klasifikovani kada se puste kroz dati klasifikator.
	Ovo kao posledicu ima da je dobijeni klasifikator neispravan, \v sto dalje uzrokuje da se dobijeni autoenkoder ne mo\v ze koristiti, jer je treniran na neispravnom klasifikatoru.
	
	Prethodno diskutovani proces treniranja, kao \v sto smo videli, sadr\v zi mane i probleme koji ga \v cine neupotrebljivim, me\dj utim on postavlja dobru osnovu u na\v cinu i smeru u kojem treba da se razmi\v slja o ovom problemu.
	Ono \v sto smo uvideli iz prethodne diskusije je da je za pravljenje ispravnog su\v stinskog autoenkodera potreban ispravan klasifikator, a ma\v sinsko u\v cenje i neuronske mre\v ze se bave temom klasifikatora ve\'c dugo vremena i znamo kako se on mo\v ze napraviti.
	Kada imamo istreniran klasifiaktor mo\v zemo da iskoristimo ideju da spojimo autoenkoder i klasifikator i treniramo novi model kao jedan veliki klasifikator, me\dj utim moramo da zamrznemo sve parametre ve\'c istreniranog klasifiaktora, jer samo tako mo\v zemo da budemo sigurni da \'ce istreniran klasifikator biti ispravan tokom celog treninga i da \'ce se izbe\'ci problem iz prethodnog primera.
	Realizacija ovoga je po prilici veoma prosta, i dalje se pu\v sta nazadna propagacija\footnote{algoritam koji se koristi za treniranje neuronskih mre\v za} kroz celu mre\v zu, ali uz izmenu da menja samo parametre autoenkodera, dok parametri istreniranog klasifikatora preska\v ce kada na njih do\dj e red da se izmene.
	Dati autoenkoder se na ovaj na\v cin trenira da preslikava ulaz na rezultat koji se klasifikacijom kroz dati istrenirani autoenkoder svrstava u tra\v zenu klasu.
	
	Iako je prethodna ideja veoma blizu re\v senju koje problem zahteva, ona ipak ima jednu manu. 
	Ako se prisetimo pri\v ce o su\v stini podataka, rekli smo da se originalni podaci i rezultat autoenkodera trebaju klasifikovati u istu klasu, a nema garancije da \'ce istrenirani klasifikator ispravno klasifikovati originalne podatke, dok mi treniramo autoenkoder kao da je ovo slu\v caj.
	Iz ovog razloga treba da napravimo izmenu onoga \v cemu veliki klasifikator, koji se sastoji od autoenkodera i istreniranog klasifikatora, cilja.
	Umesto da vodi trening ka klasi kojom su originalni podaci labelovani treba da cilja na distribuciju verovatno\'ce\footnote{klasifikator preslikava ulazne podatke na distribuciju verovatno\'ce koja govori sa kojom verovatno\'com podaci pripadaju svakoj od mogu\'cih klasa} koja se dobije kada se originalni podaci puste kroz istrenirani klasifikator.
	
	Diskutovano re\v senje je, kao \v sto vidimo, ispravno, ali je ra\v cunski veoma zahtevno i bilo bi idealno ako bi mogao da se na\dj e na\v cin da se dati proces ubrza.
	Ono \v sto mo\v ze da se uradi po ovom pitanju je da popustimo uslov i ne prolazimo kroz ceo istrenirani klasifikator tokom treninga, nego da idemo samo do odre\dj enog sloja datog klasifikatora i treniramo autoenkoder tako da vrednosti u datom sloju kada se rezultati puste kroz klasifikator budu iste kao i u slu\v caju originalnih podataka.
	Ono \v sto je interesantno je da je na\v cin treniranja tradicionalnih autoenkodera i prethodno diskutovano re\v senje ustvari specijalan slu\v caj ovog pristupa: ako se uzme samo ulazni sloj mre\v ze, dobija se tradicionalno re\v senje, dok ako se uzme ceo autoenkoder dobija se originalno re\v senje izneseno u ovom radu. 
	Realizacija ove ideje je analogna originalnom re\v senju, me\dj utim prilikom izbora ovog pristupa treba biti svestan da on sa sobom nosi manu da u op\v stem slu\v caju \v sto se uzme manji broj slojeva da \'ce rezultat klasifikacije podataka dobijenih autoenkoderom biti dalje od rezultata klasifikacije originalnih podataka.
	
	Svi autoenkoderi o kojima smo do sada pri\v cali ili su bili \v cisto izgledni ili \v cisto su\v stinski autoenkoderi, a ako se prisetimo definicije \ref{autoencoder-training-based-types}, u njoj smo uveli i tre\'cu vrstu koja kombinuje ove dve diskutovane ideje. 
	Kombinovanje ove dve ideje se mo\v ze izvesti na veoma jednostavan na\v cin: ako imamo funkciju distorcije $\Delta_1 : \mathbb{A} \to \mathbb{B}_1$ specifi\v cnu za treniranje izglednog autoenkodera, funkciju distorcije $\Delta_2 : \mathbb{A} \to \mathbb{B}_2$ specifi\v cnu za treniranje su\v stinskog autoenkodera, kao i binarni operator $\oplus : \mathbb{B}_1 \times \mathbb{B}_2 \to \mathbb{B}$ mo\v zemo definisati funkciju distorcije $\Delta' : \mathbb{A} \to \mathbb{B} : x \to \Delta_1 (x) \oplus \Delta_2 (x)$ koja je specifi\v cna za treniranje izgledno-su\v stinskog autoenkodera.
	Primere funkcija distorcije $\Delta_1$ i $\Delta_2$ ve\'c znamo ili smo ih diskutovali u ovom radu, dok za operator $\oplus$ mo\v zemo izabrati neki od operatora koji se naj\v ce\v s\'ce koriste, na primer operator sabiranja $+$ ili operator mno\v zenja $\cdot$.
	Ono o \v cemu treba voditi ra\v cuna prilikom primene biblioteka za rad sa neuronskim mre\v zama je da neke od njih rezultat funkcije gre\v ske dobijaju kao vi\v sedimenzionalne tenzore koji ne moraju strogo biti istih dimenzija i zato se treba primeniti funkcija koja \'ce date vrednosti da preslika na rezultate koji pripadaju skupovima na koje se mo\v ze primeniti odabrani operator.
	
	\subsection{Identi\v cnost rezultata klasifikacije pri smanjenju dimenzionalnosti podataka}
	
	Diskutovali smo su\v stinske autoenkodere i na\v cine na koje se oni mogu trenirati, slede\'ce logi\v cno pitanje je kako mo\v zemo da ocenimo kvalitet su\v stinskog autoenkodera i uporedimo dva ovakva autoenkodera da odredimo koji je bolji.
	Ideja su\v stinskih autoenkodera se fundamentalno razlikuje od gotovo svega do sada vi\dj enog u oblasti neurosnkih mre\v za i ma\v sinskog u\v cenja, s toga poznate standardne metrike nisu dobar pokazatelj kvaliteta modela ovog tipa autoenkodera.
	Jedna ideja je da ako imamo skup podataka za evaluaciju $\mathcal{Y} = \{y_1, \dots, y_k\}$ da pustimo podatke prvo kroz autoenkoder, a zatim proverimo koliki je procenat njih svrstan u klasu u koju su labelovane.
	Ovo izgleda kao dobra metrika, ali ona nije u potpunosti u skladu sa motivacijom iza su\v stine podataka.
	Ako se setimo cilj iza su\v stine podataka je da se podaci u \v sto ve\'coj meri klasifikuju u istu klasu u koju i rezultati kada se ti isti podaci puste kroz su\v stinski autoenkoder.
	Mo\v zemo pogledati definiciju \ref{identicality-of-results-of-classification-upon-dimensionality-reduction} koja defini\v se pojam identi\v cnost rezultata klasifikacije pri smanjenju dimenzionalnosti podataka, u nastavku samo identi\v cnost rezultata, koja je upravo mera ovoga do sada diskutovanog. 
	Ova mera predstavlja procenat trening vektora iz skupa $\mathcal{Y}$ koji su klasifikovani klasifikatorom $g$ u istu klasu u koju su istim klasifikatorom klasifikovani i rezultati kada se isti ti trening vektori puste kroz autoenkoder $g$.
	
	\begin{definition}[Identi\v cnost rezultata klasifikacije pri smanjenju dimenzionalnosti]
		\label{identicality-of-results-of-classification-upon-dimensionality-reduction}
		\sloppy Za autoenkoder $f : \mathbb{A}^n \to \mathbb{A}^n$ i klasifikator $g : \mathbb{A}^n \to \mathbb{B}$ (relacija $=$ mora biti definisana nad $\mathbb{B}$) koji vra\'ca klasu u kojoj se ulaz sa najve\'com veroatno\'com nalazi defini\v semo meru \emph{identi\v cnost rezultata klasifikacije pri smanjenju dimenzionalnosti podataka $\Upsilon$ nad skupom $\mathcal{Y} = \{y_1, \dots, y_k\}, \mathcal {Y} \subset \mathbb{A}^n$} kao:
		
		$$
		\Upsilon (f, g; \mathcal{Y}) = \frac{\left| \{y ~|~ y \in \mathcal{Y}~ \wedge~ g(y) = g(f(y))\}\right|}{\left| \mathcal{Y} \right|}.
		$$
	\end{definition}

	Problem kod identi\v cnosti rezultata je pitanje izbora klasifikatora koji \'cemo da koristimo u datoj metrici i kao logi\v can odgovor na ovo se daje klasifikator koji je kori\v s\'cen tokom treniranja autoenkodera.
	Me\dj utim, ovde se lako mo\v ze primetiti da se tako mo\v ze dobiti da je identi\v cnost rezultata na datom klasifikatoru visoka, a na ostalim klasifikatorima koji se koriste za isti klasifikacioni problem veoma mala, to jest odre\dj eni vid \emph{overfitting}-a. 
	Ovo budi mnogo dublja pitanja.
	Ako se prisetimo, mi do sada jo\v s nismo formalno definisali su\v stinu podataka i najbli\v ze \v sto smo rekli o njoj je da ako se iz podataka uklone sve osobine koje nisu bitne za klasifikaciju da ono \v sto ostaje je ustvari su\v stina podataka.
	Ono \v sto je ovde problem je ako postoje dva klasifikatora $g_1$ i $g_2$ koji redom tragaju za skupovima osobina $\mathcal{G}_1$ i $\mathcal{G}_2$ nad odre\dj enim podacima u svrhu pronala\v zenja odgovora na isto klasifikaciono pitanje i na to sve jo\v s va\v zi da je $\mathcal{G}_1 \cap \mathcal{G}_2 \ne \emptyset$.
	Preneseno re\v cima, ovo zna\v ci da dati klasifikatori tragaju za razli\v citim osobinama u svrhu klasifikacije podataka, \v sto bi zna\v cilo da su\v stina podataka nije jedinstvena izme\dj u klasifikatora koji re\v savaju isti klasifikacioni problem.
	Ovaj problem izlazi iz okvira ovoga rada i s toga ostaje otvoreno pitanje, a ako je odgovor potvrdan, \v sto je verovatno i slu\v caj, treba razmisliti o na\v cinu da se su\v stina iz ugla vi\v se klasifikatora mo\v ze spojiti u jednu celinu.
	Do trenutka dok se ne odgovori na ova pitanja svako kori\v s\'cenje su\v stinskih autoenkodera sa klasifikatorima sa kojima dati autoenkoder nije treniran je nepreporu\v cljivo.
	
	\subsection{Izdvajanje su\v stine podataka}
	
	Do sada je bilo mnogo pri\v ce o su\v stini podataka, ali je njeno razumevanje, u smislu da mo\v zemo pogledati podatke i uvideti \v sta je njihova su\v stina, veoma te\v sko.
	Me\dj utim su\v stinski autoenkoderi govore o o\v cuvanju su\v stine i postavlja se pitanje da li mo\v zemo nekako ovu ideju da iskoristimo za izdvajanje su\v stine iz samih podataka.
	Ako se zamislimo kako klasifikatori rade, oni u su\v stini tragaju za odre\dj enim osobinama, \v sto bi intuitivno zna\v cilo da odbacuju one za koje vide da nemaju zna\v caja za klasifikaciju i tako kako se ide dublje u klasifikator se u su\v stini sve ve\'ci broj osobina filtrira i pri samom kraju ostaje reprezentacija koja sadr\v zi osobine ulaznih podataka koje su najbitnije za klasifikaciju.
	Ono \v sto je cilj odavde je da probamo da od ove reprezentacije dobijemo podatke na ulazu i ako je intuicija ta\v cna onda su se osobine koje nisu bitne za klasifikaciju izgubile, \v sto zna\v ci da bi rekonstruisana verzija sadr\v zala samo osobine bitne za klasifikaciju, \v sto je ustvari ono \v sto smo definisali kao su\v stinu podataka.
	Va\v zno je primetiti da se ovako gledano klasifikator pona\v sa sli\v cno enkoderu ako uzmemu prvih nekoliko slojeva klasifikatora, ali obavezno izostavimo poslednji sloj koji govori o distribuciji verovatno\'ce da  podaci pripadaju svakoj od datih klasa. 
	\v Sta vi\v se, popu\v stanjem op\v steg autoenkoder \textit{framework}-a (definicija \ref{general-autoencoder-framework}) mo\v zemo ovo uklopiti u njega.
	Cilj treninga autoenkodera se svodi na pronalazak funkcija $B \in \mathcal{B}$ i $A \in \mathcal{A}$ za koje je funkcija distorcije $\Delta$ po skupu trening vektora minimalna.
	Uzimanjem prvih nekoliko slojeva klasifikatora da igraju ulogu enkodera mi ustvari ve\'c dobijamo funkciju $B$ iz definicije i proces trenigna bi se ustvari sveo samo na pronalazak funkcije $A$. 
	Ovo se mo\v ze realizovati na sli\v can na\v cin na koji smo realizovali treniranje su\v stinskog autoenkodera uz pomo\'c unapred istreniranog klasifikatora, a to je samo zamrznemo parametre datog dela klasifikatora, dok ostatak treniramo kao i obi\v cno propagacijom u nazad.
	Za treniranje ovakve mre\v ze se mogu koristiti na\v cini treniranja izglednih, su\v stinskih, kao i izledno-su\v stinskih autoenkodera, \v sta vi\v se u nastavku rada \'ce se uporediti rezultati svakog od ovih na\v cina treniranja.
	Ovakva mre\v za \'ce se u nastavku zvati \emph{KeyFeaturesNet} ili skra\'ceno \emph{KFN}.
	
	
	
	\section*{Zaklju\v cak}
	\addcontentsline{toc}{section}{\protect{}Zaklju\v cak}
	
	\pagebreak
	\bibliographystyle{unsrt}
	\bibliography{references}
\end{document}