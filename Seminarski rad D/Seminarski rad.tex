\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[serbian]{babel}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{array}
\usepackage[a4paper]{geometry}
\usepackage{blindtext}
\usepackage{tocbibind} % Ubacuje literaturu u sadrzaj
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pgffor}
\usepackage{subcaption}

\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usetikzlibrary{positioning,calc}
\tikzset{%
	every neuron/.style={
		circle,
		draw,
		minimum size=1cm
	},
	neuron missing/.style={
		draw=none, 
		scale=4,
		text height=0.333cm,
		execute at begin node=\color{black}$\vdots$
	},
}


% makes TOC clickable
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

\title{Su\v stina podataka i njen zna\v caj za autoenkodere}
\author{Milo\v s Vujasinovi\'c}
\date{Novi Sad, jul 2020}

\graphicspath{{img}}

\newtheorem{definition}{Definicija}


% tabularx columns
\newcolumntype{Y}{>{\centering\arraybackslash}X}

% Variables
\newcommand{\titlelogosize}{2.4cm}

% Adding pagebreak before section
\let\oldsection\section
\renewcommand\section{\clearpage\oldsection}

% custom make title
\makeatletter         
\renewcommand\maketitle{
	\thispagestyle{empty}
	\begin{tabularx}{\textwidth}{m{\titlelogosize}Ym{\titlelogosize}}	\includegraphics[width=\linewidth]{./pmflogo} &
		\begin{tabular}{@{}c@{}}
			УНИВЕРЗИТЕТ У НОВОМ САДУ\\
			ПРИРОДНО-МАТЕМАТИЧКИ ФАКУЛТЕТ\\
			ДЕПАРТМАН ЗА МАТЕМАТИКУ И\\
			ИНФОРМАТИКУ
		\end{tabular} &
		\includegraphics[width=\linewidth]{./unslogo}
	\end{tabularx}
	
	\vfill
	
	\begin{center}
		\LARGE
		\textbf{\@title}
		
		\medskip
		
		Seminarski rad
	\end{center}
	
	\vfill
	
	\begin{flushright}
		\large
		\textbf{\@author}
	\end{flushright}
	
	\bigskip
	\bigskip
	
	\begin{center}
		\large
		\@date
	\end{center}
	\pagebreak
	
	\clearpage
	\pagenumbering{arabic} 
}
\makeatother

\newcommand{\subfigureimg}[3]{
\begin{subfigure}{.3\textwidth}
	\centering
	\includegraphics[width=.9\textwidth]{img/#1}
	\caption{#2}
	\label{#3}
\end{subfigure}
}

%Making table aand figure have same counter
\makeatletter 
\let\c@table\c@figure
\let\c@lstlisting\c@figure
\makeatother

\begin{document}
	\maketitle
	
	\tableofcontents
	
	\section*{Uvod}
	\addcontentsline{toc}{section}{\protect{}Uvod}
	
	Autoenkoderi su ve\' c godinama zlatni standard u smanjenju dimenzionalnosti podataka. Na\v cin na koji rade se pokazao kao veoma efikasan u otklanjanju \v suma i dopunjavanju podataka koji su o\v ste\' ceni. Motivisan datim primerima, ovaj rad poku\v sava da prika\v ze novi na\v cin gledanja na smanjene dimenzionalnosti podataka ma\v sinskim u\v cenjem. Zatim, primenom iznesenih ideja i nekih od principa koji se nalaze u osnovi autoenkodera se uvodi model neuronske mre\v ze koji ima za cilj da iz podataka koji se prosle\dj uju modelu izdvoji najbitnije odlike za klasifikaciju. Kroz ovaj postupak se tako\dj e razmatraju novi na\v cini treniranja i evaluacije modela, a na kraju se rezultati datog modela porede sa rezultatima tradicionalnih autoenkodera.
	
	\section{Su\v stinski autoenkoderi}
	
	\subsection{Su\v stina podataka i problem sa tradicionalnim autoenkoderom}
	
	Autoenkoderi su neuronske mre\v ze koje se treniraju da opona\v saju identi\v cko preslikavanje, to jest da ulaz preslikava na samog sebe. 
	Ovo samo po sebi ne izgleda veoma korisno, me\dj utim autoenkoderi, pored navedenih, sadr\v ze i osobinu da je broj neurona u najmanjem skrivenom sloju mre\v ze manji od broja neurona ulaznog i izlaznog sloja. 
	Ovo za implikaciju ima da se prilikom prolaska podataka kroz mre\v zu u datom sloju nalazi reprezentacija podataka za \v cije je predstavljanje potreban manji broj memorijskih jedinica od originalnih podataka i ovo mo\v ze da varira od nekoliko jedinica, pa do redova veli\v cina manje. 
	Dati skriveni sloj se naziva \emph{usko grlo} (engl. \emph{bottle neck}) i on deli neuronsku mre\v zu na dva dela: od ulaza do sebe i od sebe do izlaza. 
	Ovi delovi se redom nazivaju \emph{enkoder} i \emph{dekoder}, a oni se koriste za preslikavanje originalnih podataka u reprezentaciju smanjenih dimenzija i nazad. U nastavku rada \'ce se autoenkoderi definisati na formalniji na\v cin, ali za trenutne potrebe dato obja\v snjenje je dovoljno.
	
	Prilikom treniranja autoenkodera se gre\v ska ra\v cuna kao gre\v ska izme\dj u svakog neurona ulaza i njemu odgovaraju\'ceg neurona izlaza. 
	Ovo je veoma intuitivno re\v senje ako \v zelimo da rezultati na izlazu budu naizgled \v sto sli\v cniji ulaznim podacima. 
	Me\dj utim dato preslikavanje je u praksi gotovo nemogu\'ce napraviti tako da bude bude savr\v seno.
	Kao posledica dolazi do izvesne gre\v ske izme\dj u originalnih podataka i rezultata mre\v ze koji mo\v zda ne izgledaju zna\v cajno ili su nam gotovo neprimetni, ali, teoretski, mogu zna\v cajno da uti\v cu na rezultate klasifikacije ako bi se rezultati autoenkodera pustili kroz klasifikator i uporedili sa rezultatima klasifikacije originalnih podataka.
	Na primer, ako imamo klasifikator koji traga za odlikom koja je sadr\v zana u veoma malom broju memorijskih jedinica u pore\dj enju sa veli\v cinom jedne instance podataka, a autonkoder zaklju\v ci da bi uvr\v stavanje date odlike samo pove\'calo gre\v sku, jer bi njenim uvr\v stavanjem bilo onemogu\'ceno uvr\v stavanje neke druge odlike koja je iz pogleda gre\v ske autoenkodera va\v znija.
	Kao posledica, redukcija podataka ovim postupkom je ne potencijalno samo beskorisna, nego i \v cini podatke neupotrebljivim ako \v zelimo da ih klasifikujemo ili, jo\v s gore, koristimo za treniranje klasifikatora.
	
	Diskutovani problem svakako postavlja pitanje kako se mo\v ze prevazi\'ci, ali pre davanja odgovora na njega, potrobno je da defini\v semo podelu koja \'ce da omogu\'ci uo\v cavanje ovog problema jasnijim i lak\v sim. 
	Iz prikazanog rezonovanja se jasno mo\v ze uvideti da se autoenkoderi mogu trenirati ili u svrhu o\v cuvanja izgleda podataka ili u svrhu o\v cuvanja onoga \v sto podaci ustvari jesu, odnosno njihove \emph{su\v stine}.
	Treba imati u vidu da podaci imaju gotovo bezbroj razli\v citih su\v stina u zavisnosti od toga iz kog se ugla posmatraju, to jest da se u zavisnosti od pitanja koje se postavlja o njima njihova su\v stina menja i odatle mo\v zemo da spojimo ideju o su\v stini sa problemom klasifikacije tako \v sto \'cemo re\'ci da pitanje koje postavlja klasifikacioni problem odre\dj uje su\v stinu podataka o kojoj u datom trenutku pri\v camo.
	Ove ideje se koriste za podelu definisanu u definiciji \ref{autoencoder-training-based-types}. Iz ove definicije je jasno da su tradicionalni autoenkoderi ustvari izgledni autoenkoderi, dok autoenkoderi kojima se te\v zi u svrhu prevazila\v zenja diskutovanog problema su su\v stinski autoenkoderi, mada se i za njih, u zavisnosti od implementacije, postavlja pitanje koliko dobro mogu da o\v cuvaju odlike bitne za klasifikaciju.
	
	\begin{definition}[Podela autoenkodera po na\v cinu treniranja]
		\label{autoencoder-training-based-types}
		Autoenkoder za \v cije se treniranje koristi znanje o samim podacima se naziva \emph{izgledni autoenkoder}, dok autoenkoder za \v cije se treniranje koristi znanje o klasama kojima podaci pripadaju se naziva \emph{su\v stinski autoenkoder}. Treba imati u vidu da date osobine nisu me\dj usobno disjunktne, pa postoji i \emph{izgledno-su\v stinski autoenkoder}.
	\end{definition}


	\subsection{Treniranje su\v stinskih autoenkodera}
	\label{treniranje-sustinskih-autoenkodera}
	
	Da bismo razmatrali kako se klase kojima podaci pripadaju mogu uklju\v citi u treniranje autoenkodera defini\v simo prvo autoenkodere na formalniji na\v cin. 
	Za ovo \'cemo iskoristiti definiciju \ref{general-autoencoder-framework} preuzetu iz \cite[Poglavlje 2]{pmlr-v27-baldi12a}.
	
	\begin{definition}[Op\v sti autoenkoder \textit{framework} \cite{pmlr-v27-baldi12a}]
		\label{general-autoencoder-framework}
		\sloppy $n/p/n$ autoenkoder je definisan kao $t$-torka $n, p, m, \mathbb{F}, \mathbb{G}, \mathcal{A}, \mathcal{B}, \mathcal{X}, \Delta$ gde va\v zi:
		\begin{enumerate}
			\addtolength{\itemindent}{1em}
			\item $\mathbb{F}$ i $\mathbb{G}$ su skupovi.
			\item $n$ i $p$ su pozitivni celi brojevi. (Autor razmatra slu\v caj kada je $0 < p < n$ \v sto se podrazumeva prilikom pravljenja autoenkodera)
			\item $\mathcal{A}$ je klasa funkcija koje preslikavaju iz  $\mathbb{G}^p$ na $\mathbb{F}^n$
			\item $\mathcal{B}$ je klasa funkcija koje preslikavaju iz $\mathbb{F}^n$ na $\mathbb{G}^p$
			\item $\mathcal{X} = \{x_1, \ldots, x_m\}$ je skup od $m$ (trening) vektora u $\mathbb{F}^n$. Ako se radi o spolja\v snjem skupu ciljeva u $\mathbb{F}^n$ ozna\v cavamo ga sa $\mathcal{Y} = \{y_1, \ldots, y_n\}$.
			\item $\Delta$ je funkcija razli\v citosti ili distorcije definisana nad $\mathbb{F}^n$
		\end{enumerate}
	\end{definition}
	
	Da bismo bolje razumeli datu definiciju i bli\v ze se upoznali sa terminologijom koja \'ce se koristiti u nastavku pro\dj imo kroz sve elemente date $t$-torke koja se koristi za predstavljanje autoenkodera. 
	Kao \v sto smo ranije pri\v cali ideja iza autoenkodera je da nau\v ci da slika ulaz na samog sebe, a da pri tome postoji skriveni sloj koji je manjih dimenzija od samog ulaza i na ovaj na\v cin se posti\v ze smanjenje dimenzionalnosti podataka.
	$n$ je ni\v sta manje nego broj dimenzija ulaza kao i izlaza, dok je $p$ broj dimenzija \emph{uskog grla}\footnote{sloj u kome se prilikom prolaza podataka kroz autoenkoder nalazi reprezentacija podataka smanjenih dimenzija}, po\v sto se cilja da je broj dimenzija uskog grla manji od broja dimenzija ulaza zato se i pretpostavlja da je $0 < p < n$. 
	Svaka memorijska jedinica u ulaznom i izlaznom sloju, odnosno uskom grlu sadr\v zi podatke koji redom pripadaju skupovima $\mathbb{F}$ i $\mathbb{G}$. $\forall B \in \mathcal{B}$ je funkcija koja preslikava ulazne podatke u reprezentaciju smanjenih dimenzija i ona se naziva \emph{enkoder}. $\forall A \in \mathcal{A}$ radi obrnut proces od funkcija iz $\mathcal{B}$ i preslikava reprezentaciju smanjenih dimenzija na izlaz i ova funkcija se naziva \emph{dekoder}.
	Treba primetiti da se ovde delovi neuronske mre\v ze posmatraju kao funkcije, jer oni to i jesu, i ova notacija \'ce se u nastavku koristiti kada se govori o neuronskim mre\v zama i njenim delovima.
	Za treniranje autoenkodera su potrebni podaci i skup trening podataka je u datoj definiciji ozna\v cen sa $\mathcal{X}$.
	Me\dj utim, podaci se ne koriste samo za treniranje i mo\v zemo imati podatke koji se koriste za evaluaciju ili podatke koji se pu\v staju kroz autoenkoder u cilju smanjenja dimenzionalnosti. 
	Ovakvi skupovi podataka su u definiciji ozna\v ceni sa $\mathcal{Y}$. 
	Na kraju, ostala je jo\v s da se objasni funkcija razli\v citosti ili distorcije $\Delta$ koja igra va\v znu ulogu u procesu treniranja autoenkodera.
	Cilj treninga autoenkodera je pronalazak funkcija $A \in \mathcal{A}$ i $B \in \mathcal{B}$ za koje je vrednost funkcije distorcije minimalna nad celim skupom trening vektora $\mathcal{X}$. 
	Funkcija distorcije $\Delta$ je primer onoga \v sto kod neuronskih mre\v za znamo kao funkcija gre\v ske ili \emph{loss} funkcija.
	
	Funkcija gre\v ske je jedna od glavnih karakteristika treninga neuronskih mre\v za i ona govori u kom smeru \'ce se treniranje kretati. 
	Kao takva ona je o\v cigledna opcija za uklju\v cenje klasa kojima podaci pripadaju u trening i na taj na\v cin dobijanje su\v stinskog autoenkodera. 
	Me\dj utim, ovo nije tako lako izvesti kao \v sto zvu\v ci. 
	Dok se kod tradicionalnih, izglednih, autoenkodera srednja kvadratna gre\v ska nudi kao idealna funkcija gre\v ske, kod su\v stinskih autoenkodera stvari su malo komplikovanije i zahtevaju se odre\dj eni trikovi u cilju dobijanja \v zeljenog efekta.
	Glavni problem je \v sto kod treniranja izglednih autoenkodera je vrednost koju mre\v za treba da proizvede poznata, dok kod su\v stinskih autoenkodera je poznata samo vrednost koja treba da se dobije kada se tra\v zeni rezultati puste kroz odre\dj enu funkciju.
	Na osnovu ovoga definicija \ref{types-of-machine-learning-training} uvodi dva na\v cina treniranja na koja se model ma\v sinskog u\v cenja mo\v ze u\v citi.
	
	\begin{definition}[Podela na\v cina treniranja modela ma\v sinskog u\v cenja]
		\label{types-of-machine-learning-training}
		Ako je prilikom treniranja modela ma\v sinskog u\v cenja poznata vrednost koja se treba dobiti kao proizvod rada modela onda se dati proces treniranja naziva \emph{direktno treniranje}. 
		Nasuprot tome, treniranje u kome je poznata samo vrednost koja se treba dobiti nakon \v sto se rezultati rada modela puste kroz odre\dj enu funkciju se naziva \emph{indirektno treniranje}.
	\end{definition}

	\begin{figure}[h!]
		\centering
		\begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
		
		\foreach \m/\l [count=\y] in {1,2,3,missing,4}
		\node [every neuron/.try, neuron \m/.try] (input-autoencoder-\m) at (0,2.5-\y) {};
		
		\foreach \m [count=\y] in {1,2,3,missing,4}
		\node [every neuron/.try, neuron \m/.try ] (output-autoencoder-\m) at (2,2.5-\y) {};
		
		\foreach \m [count=\y] in {1,2,3,missing,4}
		\node [every neuron/.try, neuron \m/.try ] (input-classificator-\m) at (4.5,2.5-\y) {};
		
		\foreach \m [count=\y] in {1,missing,2}
		\node [every neuron/.try, neuron \m/.try ] (output-classificator-\m) at (6.5,1.5-\y) {};
		
		\foreach \l [count=\i] in {1,2,3,n}
		\draw [<-] (input-autoencoder-\i) -- ++(-1,0)
		node [above, midway] {$I_\l$};
		
		\foreach \l [count=\i] in {1,2,3,n}
		\draw [<-] (input-classificator-\i) -- (output-autoencoder-\i)
		node [above, midway] {$I'_\l$};
		
		\foreach \l [count=\i] in {1,m}
		\draw [->] (output-classificator-\i) -- ++(1,0)
		node [above, midway] {$\sigma_\l$};
		
		\foreach \i in {1,...,4}
		\foreach \j in {1,...,4}
		\draw [->] (input-autoencoder-\i) -- (output-autoencoder-\j);
		
		\foreach \i in {1,...,4}
		\foreach \j in {1,...,2}
		\draw [->] (input-classificator-\i) -- (output-classificator-\j);
		
		\node [align=center, above] at (0,2) {Originalni \\podaci};
		\node [align=center, above] at (2,2) {Rezultat \\autoenkodera};
		\node [align=center, above] at (4.5,2) {Ulaz \\klasifikatora};
		\node [align=center, above] at (6.5,2) {Distribucija \\verovatno\'ce};
		
		\coordinate (autoencoder-mid) at ($(input-autoencoder-3)!.5!(output-autoencoder-3)$);
		\coordinate (classificator-mid) at ($(input-classificator-3)!.5!(output-classificator-1)$);
		
		\node[fill=white,scale=3,inner xsep=1pt,inner ysep=10mm] at (autoencoder-mid)
		{$\dots$};
		
		\node[fill=white,scale=3,inner xsep=1pt,inner ysep=10mm] at (autoencoder-mid -| classificator-mid) {$\dots$};
		
		\draw [
			thick,
			decoration={
				brace,
				mirror,
				raise=.7cm
			},
			decorate
		] (input-autoencoder-4.west) -- (output-autoencoder-4.east) 
		node [pos=0.5,anchor=north,yshift=-0.75cm] {Autoenkoder}; 
		
		\draw [
			thick,
			decoration={
				brace,
				mirror,
				raise=.7cm
			},
			decorate
		]  (input-classificator-4.west) -- (output-classificator-2.east |- input-classificator-4.west)
		node [pos=0.5,anchor=north,yshift=-0.75cm] {Klasifikator}; 
		
		\end{tikzpicture}
		\caption{Model klasifikatora dobijen spajanjem autoenkodera i manjeg klasifikatora}
		\label{autoencoder-classifier-model-combination}
	\end{figure}

	Iako su su\v stinski autoenkoderi dati kao primer indirektnog treniranja, uz pomo\'c malog trika, se mo\v ze posti\'ci da njihovo treniranje bude deo direktnog procesa treniranja.
	Po\v sto znamo da rezultat su\v stinskog autoenkodera kada se pusti kroz klasifikator treba da da nama poznatu klasu, mo\v zemo ovo svesti na istovremeno treniranje autoenkodera i klasifikatora.
	Ideja je da imamo strukturu autoenkodera i klasifikatora i da od njih napravimo novu neuronsku mre\v zu tako \v sto \'cemo staviti da ulaz prvo prolazi kroz autoenkoder, potom da izlaz autoenkodera bude ulaz u dati klasifikator, a sve zajedno treniramo kao da je jedan veliki klasifikator.
	Po\v sto znamo da je izlaz autoenkodera istih dimenzija kao i ulaz, koji je ujedno istih dimenzija kao i ulaz klasifikatora, nema problema prilikom prosle\dj ivanja rezultata autoenkodera klasifikatoru, jer su dimenzije odgovaraju\'cih slojeva jednake.
	Dijagram na kojem je prikazan jedan ovakav model se mo\v ze videti na slici \ref{autoencoder-classifier-model-combination}.
	Ova ideja iako zvu\v ci mogu\'ce ipak sa sobom nosi dva problema.
	Prvi je \v sto imamo veliki model \v sto za posledicu ima du\v ze vreme treniranja i mo\v ze da na\v skodi kvalitetu istreniranog modela.
	Drugi problem je ono \v sto \v cini dati pristup beskorisnim, ali da bismo njega shvatili moramo se vratiti na koncept su\v stine podataka i njene korelacije sa originalnim podacima.
	Su\v stina podataka je rezultat razmi\v sljanja kako iz podataka mo\v ze da se ukloni \v sto je vi\v se mogu\'ce osobina tako da rezultat klasifikacije podataka koji sadr\v ze preostale osobine bude isti kao rezultat klasifikacije originalnih podataka.
	Ovo bi zna\v cilo da ako imamo savr\v sen su\v stinski autoenkoder i znamo da se ulazni podaci klasifikakuju u odre\dj enu klasu da bi onda i rezultat autoenkodera za date podatke morao da se klasifikuje u istu klasu, kao i obrnuto.
	Rezultat prethodno opisanog procesa treniranja bi bili autoenkoder i klasifikator i iako bi rezultat klasifikacije kroz dati klasifikator podataka propu\v stenih kroz dati autoenkoder te\v zio ka klasi ka kojoj je treniran, u datom procesu treniranja ne postoji mehanizam koji obezbe\dj uje da \'ce originalni podaci biti dobro klasifikovani kada se puste kroz dati klasifikator.
	Ovo kao posledicu ima da je dobijeni klasifikator neispravan, \v sto dalje uzrokuje da se dobijeni autoenkoder ne mo\v ze koristiti, jer je treniran na neispravnom klasifikatoru.
	
	Prethodno diskutovani proces treniranja, kao \v sto smo videli, sadr\v zi mane i probleme koji ga \v cine neupotrebljivim, me\dj utim on postavlja dobru osnovu u na\v cinu i smeru u kojem treba da se razmi\v slja o ovom problemu.
	Ono \v sto smo uvideli iz prethodne diskusije je da je za pravljenje ispravnog su\v stinskog autoenkodera potreban ispravan klasifikator, a ma\v sinsko u\v cenje i neuronske mre\v ze se bave temom klasifikatora ve\'c dugo vremena i znamo kako se on mo\v ze napraviti.
	Kada imamo istreniran klasifiaktor mo\v zemo da iskoristimo ideju da spojimo autoenkoder i klasifikator i treniramo novi model kao jedan veliki klasifikator, me\dj utim moramo da zamrznemo sve parametre ve\'c istreniranog klasifikatora, jer samo tako mo\v zemo da budemo sigurni da \'ce istreniran klasifikator biti ispravan tokom celog treninga i da \'ce se izbe\'ci problem iz prethodnog primera.
	Realizacija ovoga je po prilici veoma prosta, i dalje se pu\v sta nazadna propagacija\footnote{algoritam koji se koristi za treniranje neuronskih mre\v za} kroz celu mre\v zu, ali uz izmenu da menja samo parametre autoenkodera, dok se parametri istreniranog klasifikatora preska\v cu kada na njih do\dj e red da budu izmenjeni.
	Dati autoenkoder se na ovaj na\v cin trenira da preslikava ulaz na rezultat koji se klasifikacijom kroz dati istrenirani autoenkoder svrstava u tra\v zenu klasu.
	
	Iako je prethodna ideja veoma blizu re\v senju koje problem zahteva, ona ipak ima jednu manu. 
	Ako se prisetimo pri\v ce o su\v stini podataka, rekli smo da se originalni podaci i rezultat autoenkodera trebaju klasifikovati u istu klasu, a nema garancije da \'ce istrenirani klasifikator ispravno klasifikovati originalne podatke, dok mi treniramo autoenkoder kao da je ovo slu\v caj.
	Iz ovog razloga treba da napravimo izmenu onoga \v cemu veliki klasifikator, koji se sastoji od autoenkodera i istreniranog klasifikatora, cilja.
	Umesto da vodi trening ka klasi kojom su originalni podaci labelovani treba da cilja na distribuciju verovatno\'ce\footnote{klasifikator preslikava ulazne podatke na distribuciju verovatno\'ce koja govori sa kojom verovatno\'com podaci pripadaju svakoj od mogu\'cih klasa} koja se dobije kada se originalni podaci puste kroz istrenirani klasifikator.
	
	Diskutovano re\v senje je, kao \v sto vidimo, ispravno, ali je ra\v cunski veoma zahtevno i bilo bi idealno ako bi mogao da se na\dj e na\v cin da se dati proces ubrza.
	Ono \v sto mo\v ze da se uradi po ovom pitanju je da popustimo uslov i ne prolazimo kroz ceo istrenirani klasifikator tokom treninga, nego da idemo samo do odre\dj enog sloja datog klasifikatora i treniramo autoenkoder tako da vrednosti u datom sloju kada se rezultati puste kroz klasifikator budu iste kao i u slu\v caju originalnih podataka.
	Ono \v sto je interesantno je da su na\v cin treniranja tradicionalnih autoenkodera i prethodno diskutovano re\v senje ustvari specijalni slu\v cajevi ovog pristupa: ako se uzme samo ulazni sloj mre\v ze, dobija se tradicionalno re\v senje, dok ako se uzme ceo autoenkoder dobija se originalno re\v senje izneseno u ovom radu. 
	Realizacija ove ideje je analogna originalnom re\v senju, me\dj utim prilikom izbora ovog pristupa treba biti svestan da on sa sobom nosi manu da u op\v stem slu\v caju \v sto se uzme manji broj slojeva da \'ce rezultat klasifikacije podataka dobijenih autoenkoderom biti dalje od rezultata klasifikacije originalnih podataka.
	
	Svi autoenkoderi o kojima smo do sada pri\v cali ili su bili \v cisto izgledni ili \v cisto su\v stinski autoenkoderi, a ako se prisetimo definicije \ref{autoencoder-training-based-types}, u njoj smo uveli i tre\'cu vrstu koja kombinuje ove dve diskutovane ideje. 
	Kombinovanje ove dve ideje se mo\v ze izvesti na veoma jednostavan na\v cin: ako imamo funkciju distorcije $\Delta_1 : \mathbb{A} \to \mathbb{B}_1$ specifi\v cnu za treniranje izglednog autoenkodera, funkciju distorcije $\Delta_2 : \mathbb{A} \to \mathbb{B}_2$ specifi\v cnu za treniranje su\v stinskog autoenkodera, kao i binarni operator $\oplus : \mathbb{B}_1 \times \mathbb{B}_2 \to \mathbb{B}$ mo\v zemo definisati funkciju distorcije $\Delta' : \mathbb{A} \to \mathbb{B} : x \to \Delta_1 (x) \oplus \Delta_2 (x)$ koja je specifi\v cna za treniranje izgledno-su\v stinskog autoenkodera.
	Primere funkcija distorcije $\Delta_1$ i $\Delta_2$ ve\'c znamo ili smo ih diskutovali u ovom radu, dok za operator $\oplus$ mo\v zemo izabrati neki od operatora koji se naj\v ce\v s\'ce koriste, na primer operator sabiranja $+$ ili operator mno\v zenja $\cdot$.
	Ono o \v cemu treba voditi ra\v cuna prilikom primene biblioteka za rad sa neuronskim mre\v zama je da neke od njih rezultat funkcije gre\v ske dobijaju kao vi\v sedimenzionalne tenzore koji ne moraju strogo biti istih dimenzija i zato se treba primeniti funkcija koja \'ce date vrednosti da preslika na rezultate koji pripadaju skupovima na koje se mo\v ze primeniti odabrani operator.
	
	\subsection{Identi\v cnost rezultata klasifikacije pri smanjenju dimenzionalnosti podataka}
	
	Diskutovali smo su\v stinske autoenkodere i na\v cine na koje se oni mogu trenirati, slede\'ce logi\v cno pitanje je kako mo\v zemo da ocenimo kvalitet su\v stinskog autoenkodera i uporedimo dva ovakva autoenkodera da odredimo koji je bolji.
	Ideja su\v stinskih autoenkodera se fundamentalno razlikuje od gotovo svega do sada vi\dj enog u oblasti neurosnkih mre\v za i ma\v sinskog u\v cenja, s toga poznate standardne metrike nisu dobar pokazatelj kvaliteta modela ovog tipa autoenkodera.
	Jedna ideja je da ako imamo skup podataka za evaluaciju $\mathcal{Y} = \{y_1, \dots, y_k\}$ i da pustimo podatke prvo kroz autoenkoder, a zatim proverimo koliki je procenat njih svrstan u klasu u koju su labelovane.
	Ovo izgleda kao dobra metrika, ali ona nije u potpunosti u skladu sa motivacijom iza su\v stine podataka.
	Ako se setimo cilj iza su\v stine podataka je da se originalni podaci u \v sto ve\'coj meri klasifikuju u istu klasu u koju i rezultati kada se ti isti podaci puste kroz su\v stinski autoenkoder.
	Mo\v zemo pogledati definiciju \ref{identicality-of-results-of-classification-upon-dimensionality-reduction} koja defini\v se pojam identi\v cnost rezultata klasifikacije pri smanjenju dimenzionalnosti podataka, u nastavku samo identi\v cnost rezultata, koja je upravo mera ovoga do sada diskutovanog. 
	Ova mera predstavlja procenat trening vektora iz skupa $\mathcal{Y}$ koji su klasifikovani klasifikatorom $g$ u klasu u koju su istim klasifikatorom klasifikovani i rezultati rada autoenkodera $f$ kada se ti trening vektori puste kroz njega.
	
	\begin{definition}[Identi\v cnost rezultata klasifikacije pri smanjenju dimenzionalnosti]
		\label{identicality-of-results-of-classification-upon-dimensionality-reduction}
		\sloppy Za autoenkoder $f : \mathbb{A}^n \to \mathbb{A}^n$ i klasifikator $g : \mathbb{A}^n \to \mathbb{B}$ (relacija $=$ mora biti definisana nad $\mathbb{B}$) koji vra\'ca klasu u kojoj se ulaz sa najve\'com veroatno\'com nalazi defini\v semo meru \emph{identi\v cnost rezultata klasifikacije pri smanjenju dimenzionalnosti podataka $\Upsilon$ nad skupom $\mathcal{Y} = \{y_1, \dots, y_k\}, \mathcal {Y} \subset \mathbb{A}^n$} kao:
		
		$$
		\Upsilon (f, g; \mathcal{Y}) = \frac{\left| \{y ~|~ y \in \mathcal{Y}~ \wedge~ g(y) = g(f(y))\}\right|}{\left| \mathcal{Y} \right|}.
		$$
	\end{definition}

	Problem kod identi\v cnosti rezultata je pitanje izbora klasifikatora koji \'cemo da koristimo u datoj metrici i kao logi\v can odgovor na ovo se nudi klasifikator koji je kori\v s\'cen tokom treniranja autoenkodera.
	Me\dj utim, ovde se lako mo\v ze primetiti da se tako mo\v ze dobiti da je identi\v cnost rezultata na datom klasifikatoru visoka, a na ostalim klasifikatorima koji se koriste za isti klasifikacioni problem veoma mala, to jest odre\dj eni vid \emph{overfitting}-a. 
	Ovo budi mnogo dublja pitanja.
	Ako se prisetimo, mi do sada jo\v s nismo formalno definisali su\v stinu podataka i najbli\v ze \v sto smo rekli o njoj je da ako se iz podataka uklone sve osobine koje nisu bitne za klasifikaciju da ono \v sto ostaje je ustvari su\v stina podataka.
	Ono \v sto je ovde problem je ako postoje dva klasifikatora $g_1$ i $g_2$ koji redom tragaju za skupovima osobina $\mathcal{G}_1$ i $\mathcal{G}_2$ nad odre\dj enim podacima u svrhu pronala\v zenja odgovora na isto klasifikaciono pitanje i na to sve jo\v s va\v zi da je $\mathcal{G}_1 \cap \mathcal{G}_2 \ne \emptyset$.
	Preneseno re\v cima, ovo zna\v ci da dati klasifikatori tragaju za razli\v citim osobinama u svrhu klasifikacije podataka, \v sto bi zna\v cilo da su\v stina podataka nije jedinstvena izme\dj u klasifikatora koji re\v savaju isti klasifikacioni problem.
	Ovaj problem izlazi iz okvira ovoga rada i s toga ostaje otvoreno pitanje, a ako je odgovor potvrdan, \v sto je verovatno i slu\v caj, treba razmisliti o na\v cinu da se razli\v cite su\v stine iz ugla vi\v se klasifikatora mogu spojiti u jednu.
	Do trenutka dok se ne odgovori na ova pitanja svako kori\v s\'cenje su\v stinskih autoenkodera sa klasifikatorima nad kojima dati autoenkoder nije treniran je nepreporu\v cljivo.
	
	\subsection{Izdvajanje su\v stine podataka}
	
	Do sada je bilo mnogo pri\v ce o su\v stini podataka, ali je njeno razumevanje, u smislu da mo\v zemo pogledati podatke i uvideti \v sta je njihova su\v stina, veoma te\v sko.
	Me\dj utim su\v stinski autoenkoderi govore o o\v cuvanju su\v stine i postavlja se pitanje da li mo\v zemo nekako ovu ideju da iskoristimo za izdvajanje su\v stine iz samih podataka.
	Ako se zamislimo kako klasifikatori rade, oni u su\v stini tragaju za odre\dj enim osobinama, \v sto bi intuitivno zna\v cilo da odbacuju one za koje vide da nemaju zna\v caja za klasifikaciju i tako kako se ide dublje u klasifikator se u su\v stini sve ve\'ci broj osobina filtrira i pri samom kraju ostaje reprezentacija koja sadr\v zi osobine ulaznih podataka koje su najbitnije za klasifikaciju.
	Ono \v sto je cilj odavde je da probamo da od ove reprezentacije dobijemo podatke na ulazu i ako je intuicija ta\v cna onda su se osobine koje nisu bitne za klasifikaciju izgubile, \v sto zna\v ci da bi rekonstruisana verzija sadr\v zala samo osobine bitne za klasifikaciju, \v sto je ustvari ono \v sto smo definisali kao su\v stinu podataka.
	Va\v zno je primetiti da se ovako uzetih prvih nekoliko slojeva klasifikatora pona\v sa sli\v cno enkoderu, ali obavezno trebamo izostaviti poslednji sloj koji govori o distribuciji verovatno\'ce da  podaci pripadaju svakoj od datih klasa. 
	\v Sta vi\v se, popu\v stanjem op\v steg autoenkoder \textit{framework}-a (definicija \ref{general-autoencoder-framework}) mo\v zemo ovo uklopiti u njega.
	Cilj treninga autoenkodera se svodi na pronalazak funkcija $B \in \mathcal{B}$ i $A \in \mathcal{A}$ za koje je funkcija distorcije $\Delta$ po skupu trening vektora minimalna.
	Uzimanjem prvih nekoliko slojeva klasifikatora da igraju ulogu enkodera mi ustvari ve\'c dobijamo funkciju $B$ iz definicije i proces trenigna bi se ustvari sveo samo na pronalazak funkcije $A$. 
	Ovo se mo\v ze realizovati na sli\v can na\v cin na koji smo realizovali treniranje su\v stinskog autoenkodera uz pomo\'c unapred istreniranog klasifikatora, a to je samo zamrznemo parametre datog dela klasifikatora, dok ostatak treniramo kao i obi\v cno propagacijom u nazad.
	Za treniranje ovakve mre\v ze se mogu koristiti na\v cini treniranja izglednih, su\v stinskih, kao i izledno-su\v stinskih autoenkodera i, \v sta vi\v se, u nastavku rada \'ce se uporediti rezultati svakog od ovih na\v cina treniranja.
	Ovakva mre\v za \'ce se zvati \emph{KeyFeaturesNet} ili skra\'ceno \emph{KFN}.
	
	
	\section{Rezultati testiranja}
	
	U ovom poglavlju \'cemo prikazati kakav uticaj razli\v citi na\v cini treniranja imaju na metrike \emph{KeyFeaturesNet}-a, kao i videti kako se njihovi rezultati porede sa rezultatima tradicionalnih (izglednih) autoenkodera. 
	Svakom modelu treniranom posebnim na\v cinom u\v cenja kao i tradicionalnom autoenkoderu su dodeljene posebne identifikacione oznake radi lak\v seg prepoznavanja. 
	Tradicionalni autoenkoder \'cemo ozna\v cavati kao \textbf{AE}. 
	Za treniranje \emph{KFN}-a koristi\'cemo sve do sada  analizirane na\v cine u\v cenja. 
	Prvi od njih je na\v cin na koji se treniraju izgledni autoenkoderi, a to je primena srednje kvadratne gre\v ske izme\dj u rezultata \emph{KFN}-a i originalnih podataka kao funkcije distorcije.
	Modelu koji koristi ovu funkciju za treniranje dodeli\'cemo oznaku \textbf{KFN\_MSE}.
	Zatim, prisetimo se na\v cina za treniranje su\v stinskih autoenkodera koje smo diskutovali u podpoglavlju \ref{treniranje-sustinskih-autoenkodera}.
	Prvi od njih je da treniramo \emph{KFN} tako da distribucija verovatno\'ce klasifikacije rezultata \emph{KFN}-a bude jednaka distristribuciji verovatno\'ce klasifikacije originalnih podataka.
	Ako se prisetimo realizacija ovoga je i\v sla spajanjem tada autoenkodera i istreniranog klasifikatora u novi model koji se trenira kao jedan veliki klasifikator, ali kome su parametri delova modela konstantni i njihova izmena se preska\v ce tokom propagacije u nazad.
	Za treniranje velikog klasifikatora se kao funkcija gre\v ske uzima kros entropija (engl. \emph{cross entropy}) izme\dj u distribucija verovatno\'ce klasifikacije istreniranim klasifikatorom rezultata autoenkodera i originalnih podataka. 
	Po nazivu funkcije koja se koristi ovaj model \'cemo ozna\v citi sa \textbf{KFN\_CE}. 
	Slede\'ce \v sto smo rekli je da ne moramo pu\v stati vrednosti kroz ceo klasifikator, ve\'c da mo\v zemo i\'ci do odre\dj enog sloja i da pode\v savamo vrednosti parametara tako da vrednosti originalnih podataka i rezultata autoenkodera kada se puste kroz dati klasifikator budu jednake u datom sloju.
	Prisetimo se da se veoma sli\v cna ideja koristi za enkoder deo \emph{KFN}-a i s toga mo\v zemo odabrati isti sloj da do njega idemo i za treniranje, a kao funkciju gre\v ske uzimamo srednju kvadratnu gre\v sku izme\dj u rezultata originalnih podataka i izlaza \emph{KFN}-a kada se u istreniranom klasifikatoru puste do datog sloja. 
	Ovde treba napomenuti da se klasifikator od kojeg je uzeto prvih nekoliko slojeva kao enkoder \emph{KFN}-a koristi i tokom opisanog na\v cina treniranja.
	Model treniran ovim na\v cinom ozna\v cavamo sa \textbf{KFN\_MID}.
	Na kraju, mo\v zemo kombinovati date na\v cine treniranja izglednih i su\v stinskih autoenkodera kako bismo dobili izgledno-su\v stinski \emph{KFN} autoenkoder.
	Operator koji je u testiranju kori\v s\'cen za kombinovanje funkcija distorcije je operator sabiranja $+$ nad realnim brojevima.
	Biblioteka za rad sa neuronskim mre\v zama kori\v s\'cena u radu vrednosti funkcija gre\v ske mo\v ze da vrati kao vi\v sedimenzionalne tenzore i iz tog razloga su kori\v s\'cene funkcije redukcije kako bi se dati tenzori sveli na realne brojeve.
	U ovu svrhu kori\v s\'cena je funkcija sumiranja koja vra\'ca zbir svih elemenata datog tenzora, a pored nje je kori\v s\'cena i funkcija srednje vrednosti koja dati tenzor preslikava na srednju vrednost njegovih elemenata.
	U zavisnosti koja je funkcija redukcije kori\v s\'cena imamo redom sufikse SUM i AVG.
	Kombinacijom KFN\_MSE i KFN\_CE dobijamo model sa oznakom \textbf{KFN\_CE\_MSE} i u zavisnosti od kori\v s\'cene funkcije redukcije imamo \textbf{KFN\_CE\_MSE\_SUM} i \textbf{KFN\_CE\_MSE\_AVG}.
	Za KFN\_MSE i KFN\_MID analogno imamo model pod oznakom \textbf{KFN\_MID\_MSE} i u zavisnosti od kori\v s\'cene funkcije redukcije imamo \textbf{KFN\_MID\_MSE\_SUM} i \textbf{KFN\_MID\_MSE\_AVG}.
	
	Mere koje su kori\v s\'cene za evaluaciju klasifikatora su srednja vrednost funkcije gre\v ske, kao i ta\v cnost klasfikacije na evaluacionom skupu podataka.
	Za razliku od ovoga za autoenkoder i modele \emph{KFN}-a kori\v s\'cena je identi\v cnost rezultata klasifikacije pri smanjenju dimenzionalnosti podataka i, manje bitne, srednja vrednost funkcije gre\v ske (\v sto se jo\v s naziva \emph{loss}) i ta\v cnost klasifikacije kada se rezultati odgovoraja\'cih modela puste kroz istrenirani klasifikator.
	Za evaluaciju je kori\v s\'cen poseban skup podataka koji je nezavisan od skupa kori\v s\'cenog za treniranje bilo kog od navedenih modela.
	
	Dosta je bitno videti kako se modeli pona\v saju nad skupovima podataka razli\v cite kompleksnosti, odnosno koliko su skalabilni u zavisnosti od slo\v zenosti klasifikacionog problema koji im je dat.
	Iz ovog razloga prikazani su rezultati testiranja nad dva skupa podataka koji se razlikuju po kompleksnosti, a to su \textbf{MNIST} i \textbf{CIFAR-10}.
	Oba skupa su skupovi slika i ovo nije uop\v ste slu\v cajno, jer \'cemo mo\'ci mnogo lak\v se da uporedimo originalne podatke sa rezultatima datih modela nego \v sto bi to bio slu\v caj da se radi o skupovima podataka koji nisu slike i \v sta vi\v se glavna primena autoenkodera je u smanjenju dimenzionalnosti slika.
	Zbog osobine problema svi modeli \'ce koristiti konvolucijske strukture mre\v za koje su mnogo efikasnije u radu sa slikama nego \v sto je to slu\v caj sa potpuno povezanim slojevima.
	Za svaki od problema su modeli \emph{KFN}-a identi\v cni  i razlikuje se samo na\v cin njihovog treniranja, dok struktura datog tradicionalnog autoenkodera mo\v ze malo da varira od njih u svrhu dobijanja boljih rezultata (varijacije su na nivou broja filtera u slojevima i kori\v s\'cenju razli\v citih slojeva pri pove\'canju dimenzionalnosti koji rade isti posao uz manje izmene).
	
	\subsection{MNIST}
	
	MNIST je skup podataka koji se sastoji iz dva dela: trening skupa sa 60000 instanci i test skupa sa 10000 instanci. 
	On se sastoji od ru\v cno napisanih cifara od 0 do 9 u vidu crno-belih slika dimenzija $28 \times 28$ piksela. 
	MNIST je veoma jednostavan skup podataka koji se preporu\v cuje ljudima koji \v zele da nau\v ce tehnike i metode prepoznava\'ca osobina na podacima iz stvarnog sveta \cite{lecun-mnisthandwrittendigit-2010}.
	Kao takvog ovaj skup podataka mo\v zemo koristiti kao dobar pokazatelj da li su do sada diskutovani metodi uop\v ste primenljivi nad stvarnim podacima i koliko se dobro nose sa njima.
	
	Za ovaj skup podataka kori\v s\'cen je klasifikator sa srednjom vredno\v s\'cu funkcije gre\v ske od $0.0514$ i ta\v cno\v s\'cu klasifikacije od $0.9841$ nad evaluacionim skupom podataka.
	Svi slojevi klasifikatora sem izlaznog su uzeti u svrhu kreiranja enkoder dela \emph{KFN}-a i poslednji od datih je potpuno povezani sloj sa 20 neurona, \v sto odgovara broju neurona u uskom grlu kori\v s\'cenog tradicionalnog autoenkodera.
	
	\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}
			\hline
			Model & \begin{tabular}{@{}c@{}}\emph{Loss} pri\\ klasifikaciji\\ rezultata\\ modela\end{tabular} & \begin{tabular}{@{}c@{}}Ta\v cnost pri\\ klasifikaciji\\ rezultata\\ modela\end{tabular} & \begin{tabular}{@{}c@{}}Identi\v cnost rezultata\\ klasifikacije pri\\ smanjenju\\ dimenzionalnosti\end{tabular}\\
			\hline
			\textbf{AE} & 0.1444 & 0.9561 & 0.9638\\
			\textbf{KFN\_MSE} & 0.1723 & 0.9491 & 0.9569\\
			\textbf{KFN\_CE} & 0.0573 & 0.9827 & 0.9970\\
			\textbf{KFN\_MID} & 0.0533 & 0.9832 & 0.9985\\
			\textbf{KFN\_CE\_MSE\_SUM} & 0.1255 & 0.9675 & 0.9740\\
			\textbf{KFN\_CE\_MSE\_AVG} & 0.0554 & 0.9832 & 0.9947\\
			\textbf{KFN\_MID\_MSE\_SUM} & 0.0723 & 0.9782 & 0.9848\\
			\textbf{KFN\_MID\_MSE\_AVG} & 0.0514 & 0.9844 & 0.9981\\
			\hline
		\end{tabular}
		\caption{Rezultati evaluacije razli\v citih modela na skupu podataka MNIST}
		\label{mnist-model-results-table}
	\end{table}

	U tabeli \ref{mnist-model-results-table} mo\v zemo videti rezultate evaluacije diskutovanih modela.
	Visoki rezultati datih metrika su dobar pokazatelj da dati modeli mogu imati odre\dj eni potencijal i da su svi oni na datom problemu u rangu tradicionalnog autoenkodera i, \v sta vi\v se, ve\'cina modela se pokazala bolje u pogledu rezultata metrika od samog autoenkodera.
	Jedini model koji se pokazao lo\v sije u pogledu metrika od tradicionalnog autoenkodera je KFN\_MSE koji je jedini treniran kao \v cist izgledni autoenkoder i za koji se o\v cekivalo da od svih \emph{KFN}-ova da najlo\v sije rezultate metrika.
	Kada se na sve to jo\v s uzme u obzir da je enkoder deo datog modela fiksiran i da sadr\v zi duplo manje parametara koje mo\v ze da u\v ci u pore\dj enju sa tradicionalnim autoenkoderom nije iznena\dj uju\'ce \v sto daje lo\v sije rezultate metrika i od njega.
	\v Sto je tako\dj e zanimljivo je da KFN\_MID daje rezultate metrika veoma sli\v cne KFN\_CE \v sto je veoma iznena\dj uju\'ce, ali treba uzeti u obzir da je sloj koji je izabran za KFN\_MID odmah do izlaznog sloja klasifikatora, pa to verovatno i igra veliku ulogu u ovakvim rezultatima.

	\begin{figure}[h!]
		\centering
		\subfigureimg{mnist/original}{Originalne slike}{mnist:original}
		\foreach \path/\cap in {
			AE/AE, 
			KFN_MSE/KFN\_MSE, 
			KFN_CE/KFN\_CE, 
			KFN_MID/KFN\_MID, 
			KFN_CE_MSE_SUM/KFN\_CE\_MSE\_SUM, 
			KFN_CE_MSE_AVG/KFN\_CE\_MSE\_AVG, 
			KFN_MID_MSE_SUM/KFN\_MID\_MSE\_SUM, 
			KFN_MID_MSE_AVG/KFN\_MID\_MSE\_AVG} {\subfigureimg{mnist/\path}{\cap}{mnist:\path}}
		
		\caption{Originalne slike i slike generisane uz pomo\'c datih modela na skupu podataka MNIST}
		\label{mnist_images}
	\end{figure}

	Slede\'ce pitanje koje se postavlja je da li su ovakvi rezultati metrika posledica kvaliteta modela ili jednostavnosti skupa podataka.
	Da bismo imali bolji uvid u mogu\'cnosti modela pogledajmo slike generisane njima na slici \ref{mnist_images}.
	Ono \v sto prvo upada u oko je da \emph{KFN}-ovi trenirani na\v cinom kao su\v stinski autoenkoderi (slike \ref{mnist:KFN_CE} i \ref{mnist:KFN_MID}) daju rezultate koji su neraspoznatljivi \v sto se ipak nije o\v cekivalo i pored toga \v sto se oni ne treniraju da o\v cuvaju izgled podataka.
	Izgledno-su\v stinski autoenkoderi su se mnogo bolje pokazali i pored toga \v sto je rezultat od KFN\_MID\_MSE\_AVG veoma mutan (slika \ref{mnist:KFN_MID_MSE_AVG}), ipak se mogu uo\v citi obrisi cifara u njemu.
	Jo\v s jedna stvar koja se treba uo\v citi je uticaj koji razli\v cite funkcije redukcije primenjene pre operatora mogu imati na krajnje rezultate izgledno-su\v stinskih autoenkodera.
	Pogledajmo modele koji su koristili funkciju sume kao redukcionu funkciju i njihove rezultate na sikama \ref{mnist:KFN_CE_MSE_SUM} i \ref{mnist:KFN_MID_MSE_SUM}.
	Lako se uo\v cava da su dati rezultati mnogo jasniji od njihovih suparnika koji su koristili srednju vrednost kao redukcionu funkciju i \v ciji su rezultati prikazani na slikama \ref{mnist:KFN_CE_MSE_AVG} i \ref{mnist:KFN_MID_MSE_AVG}.
	U drugu ruku, ako pogledamo tabelu \ref{mnist-model-results-table} sa rezultatima evaluacije svakog od datih modela vide\'cemo da redukcijom uz pomo\'c funkcije srednje vrednosti se dobijaju bolji rezultati metrika od redukcije funkcijom sume.
	Ovo je dobar pokazatelj prednosti i mana koje kori\v s\'cenje razli\v citih redukcionih funkcija nosi sa sobom.
	U nastavku \'cemo pogledati da li ova pravilnost va\v zi i na kompleksnijem skupu podataka \v ciji su rezultati prikazani u podpoglavlju \ref{cifar10-section}.
	
	\subsection{CIFAR-10}
	\label{cifar10-section}
	
	CIFAR-10 je skup podataka koji se sastoji od 60000 slika u boji formata $32 \times 32$, podeljenih u 10 klasa tako da svaka klasa sadr\v zi po 6000 slika. 
	Skup je podeljen na 50000 slika za treniranje i 10000 slika za testiranje \cite{cifar10}. 
	Klase kojima ove slike mogu pripadati su avion, automobil, ptica, ma\v cka, jelen, pas, \v zaba, konj, broj i kamion.
	Iako postoje mnogo kompleksniji skupovi podataka od CIFAR-10, on je zna\v cajno slo\v zeniji od MNIST-a.
	Ne samo da su slike u boji i da su detaljnije, nego su i varijacije izme\dj u instanci iste klase mnogo ve\'ce nego \v sto je to bio slu\v caj kod MNIST-a.
	Zbog svega ovoga ovaj skup podataka se mo\v ze posmatrati kao prvi ozbiljniji izazov za do sada diskutovane modele.
	
	Klasifikator kori\v s\'cen za ovaj problem je prilikom evaluacije dao srednju vrednost funkcije gre\v ske 0.7833 i ta\v cnost klasifikacije 0.7263 nad evaluacionim skupom podataka.
	Za ovaj problem su kori\v s\'ceni \emph{dropout} slojevi radi izbegavanja \emph{overfitting}-a prilikom treniranja datog klasifikatora.
	Prilikom kori\v s\'cenja ovog klasifikatora za treniranje \emph{KFN}-a svi \emph{dropout} slojevi se trebaju isklju\v citi, jer njihovo prisustvo negativno uti\v ce na kvalitet dobijenog modela.
	Za \emph{KFN} su odabrai svi slojevi do poslednjeg konvolucijskog sloja klasifikatora kojeg od izlaznog sloja razdvaja samo jedan \emph{dropout} sloj.
	Dimenzije datog sloja su $4 \times 4 \times 16$ \v sto je isto kao i broj neurona uskog grla kori\v s\'cen u tradicionalnom autoenkoderu.
	
	\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|c|c|}
			\hline
			Model & \begin{tabular}{@{}c@{}}\emph{Loss} pri\\ klasifikaciji\\ rezultata\\ modela\end{tabular} & \begin{tabular}{@{}c@{}}Ta\v cnost pri\\ klasifikaciji\\ rezultata\\ modela\end{tabular} & \begin{tabular}{@{}c@{}}Identi\v cnost rezultata\\ klasifikacije pri\\ smanjenju\\ dimenzionalnosti\end{tabular}\\
			\hline
			\textbf{AE} & 1.4184 & 0.5282 & 0.6329\\
			\textbf{KFN\_MSE} & 2.3489 & 0.3210 & 0.3936\\
			\textbf{KFN\_CE} & 0.7910 & 0.7214 & 0.9529\\
			\textbf{KFN\_MID} & 0.7988 & 0.7231 & 0.9484\\
			\textbf{KFN\_CE\_MSE\_SUM} & 1.0444 & 0.6408 & 0.7744\\
			\textbf{KFN\_CE\_MSE\_AVG} & 0.7955 & 0.7237 & 0.9500\\
			\textbf{KFN\_MID\_MSE\_SUM} & 1.0269 & 0.6422 & 0.7804\\
			\textbf{KFN\_MID\_MSE\_AVG} & 0.8108 & 0.7178 & 0.9379\\
			\hline
		\end{tabular}
		\caption{Rezultati evaluacije razli\v citih modela na skupu podataka CIFAR-10}
		\label{cifar10-model-results-table}
	\end{table}
	
	Rezultate evaluacije datih modela mo\v zemo pogledati u tabeli \ref{cifar10-model-results-table}.
	Kao \v sto vidimo rezultati su mnogo lo\v siji u pore\dj enju sa istim nad skupom podataka MNIST.
	U ovome ulogu igra kompleksnost skupa podataka, ali i klasifikator koji nije toliko dobar kao u prethodnom slu\v caju.
	Uz sve ovo, pogledajmo sliku \ref{cifar10_images} i vide\'cemo da svi ovi rezultati samo potvr\dj uju stvari koje smo primetili na prethodnom skupu podataka. 
	KFN\_MSE je i dalje najlo\v siji u pogledu metrika, ali daje rezultate (slika \ref{cifar10:KFN_MSE}) koji su po izgledu bliski tradicionalnom autoenkoderu.
	Su\v stinski \emph{KFN}-ovi su i dalje pri vrhu po rezultatima metrika, ali slike generisane njima su absolutno neraspoznatljive.
	Ono \v sto jeste malo iznena\dj uju\'ce je da izgledno-su\v stinski \emph{KFN}-ovi koji koriste srednju vrednost kao redukcionu funkciju daju rezultate koji su u rangu su\v stinskih, ako ne i bolji, a pored toga generi\v su slike koje su za nijansu jasnije, ali koje i dalje sadr\v ze primetne artifakte.
	U drugu ruku imamo izgledno-su\v stinske \emph{KFN}-ove koji koriste funkciju sume kao redukcionu funkciju i koji generi\v su slike (slike \ref{cifar10:KFN_CE_MSE_SUM} i \ref{cifar10:KFN_MID_MSE_SUM}) koje su gotovo identi\v cne \v cisto izglednom \emph{KFN}-u (slika \ref{cifar10:KFN_MSE}), a koje daju znatno bolje vrednosti metrika koje su \v sta vi\v se ve\'ce i od metrika tradicionalnog autoenkodera. 
	
	\begin{figure}[h!]
		\centering
		\subfigureimg{cifar10/original}{Originalne slike}{cifar10:original}
		\foreach \path/\cap in {
			AE/AE, 
			KFN_MSE/KFN\_MSE, 
			KFN_CE/KFN\_CE, 
			KFN_MID/KFN\_MID, 
			KFN_CE_MSE_SUM/KFN\_CE\_MSE\_SUM, 
			KFN_CE_MSE_AVG/KFN\_CE\_MSE\_AVG, 
			KFN_MID_MSE_SUM/KFN\_MID\_MSE\_SUM, 
			KFN_MID_MSE_AVG/KFN\_MID\_MSE\_AVG} {\subfigureimg{cifar10/\path}{\cap}{cifar10:\path}}
		
		\caption{Originalne slike i slike generisane uz pomo\'c datih modela na skupu podataka CIFAR-10}
		\label{cifar10_images}
	\end{figure}
	
	\section*{Zaklju\v cak}
	\addcontentsline{toc}{section}{\protect{}Zaklju\v cak}
	
	Kroz rad smo diskutovali o su\v stini podataka i konsekvencama koje ona nosi sa sobom u pogledu smanjenja dimenzionalnosti. 
	Otvorili smo mnoge teme i zapo\v celi pri\v cu u smeru mogu\'cih re\v senja, ali smo i mnoga pitanja ostavili da \v cekaju na odgovor.
	Ono \v sto treba izvu\'ci kao poentu rada nisu principi koje rad nudi, jer su oni samo neki od potencijalno mnogih mogu\'cih, nego sama ideja o druga\v cijem pogledu na smenjenje dimenzionalnosti podataka i osnove koje rad postavlja u cilju identifikacije i prevazila\v zenja problema koje sa sobom nosi su\v stina podataka.
	Pored ovoga uveli smo model \emph{KeyFeaturesNet}-a koji je za cilj imao da nas bolje upozna sa time \v sta su\v stina podataka ustvari mo\v ze biti, ali je umesto toga mnogo ve\'cu ulogu igrao u tome da prika\v ze kako se razli\v citi tipovi treniranja me\dj usobno porede jedni sa drugima.
	Prilikom analize rezultata testiranja smo uvideli da su \v cisto su\v stinski i \v cisto izgledni autoenkoderi veoma dobri u onoj oblasti za koju su pravljeni, ali da u onoj drugoj ne daju tako dobre rezultate.
	Kao re\v senje za ovo su se ponudili izgledno-su\v stinski autoenkoderi koji u zavisnosti od funkcije redukcije koja se koristi o\v cuvavaju dobre rezultate u jednoj od datih oblasti, a ovu drugu zna\v cajno popravljaju u pore\dj enju sa svojim \v cistim suparnicima.
	
	Va\v zno je imati u vidu da \emph{KFN} izlazi iz okvira onoga \v sto se mo\v ze nazvati autoenkoderom. 
	S toga se mo\v ze re\'ci da rad nije testirao iznesene ideje nad pravim autoenkoderima i ovo se nudi kao slede\'ci korak u prou\v cavanju ideje.
	Iako smo vi\v se puta naveli da \emph{KFN} nema veliki prakti\v can zna\v caj iz rezultata testiranja se vide da sadr\v zi odre\dj eni potencijal i pitanje koje se svakako postavlja je kako se ovaj koncept fiksnog enkoder dela autoenkodera mo\v ze iskombinovati sa pravim autoenkoderima i da li takva re\v senja nude ikakva pobolj\v sanja rezultata.
	Jedna ideja za ovo je da se vrednost uskog grla ra\v cuna kao linearna kombinacija izlaza fiksnog enkodera i enkodera koji se u\v ci tokom treninga, ali ovakvu ideju treba detaljnije razmotriti i testirati da bi se videlo da li ona ima ikakav potencijal.
	
	\pagebreak
	\bibliographystyle{unsrt}
	\bibliography{references}
\end{document}