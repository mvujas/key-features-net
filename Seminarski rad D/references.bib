@InProceedings{pmlr-v27-baldi12a,
	title = 	 {Autoencoders, Unsupervised Learning, and Deep Architectures},
	author = 	 {Pierre Baldi},
	booktitle = 	 {Proceedings of ICML Workshop on Unsupervised and Transfer Learning},
	pages = 	 {37--49},
	year = 	 {2012},
	editor = 	 {Isabelle Guyon and Gideon Dror and Vincent Lemaire and Graham Taylor and Daniel Silver},
	volume = 	 {27},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Bellevue, Washington, USA},
	month = 	 {02 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf},
	url = 	 {http://proceedings.mlr.press/v27/baldi12a.html},
	abstract = 	 {Autoencoders play a fundamental role in unsupervised learning and in deep architectures for transfer learning and other tasks. In spite of their fundamental role, only linear autoencoders over the real numbers have been solved analytically. Here we present a general mathematical framework for the study of both linear and non-linear autoencoders. The framework allows one to derive an analytical treatment for the most non-linear autoencoder, the Boolean autoencoder. Learning in the Boolean autoencoder is equivalent to a clustering problem that can be solved in polynomial time when the number of clusters is small and becomes NP complete when the number of clusters is large. The framework sheds light on the different kinds of autoencoders, their learning complexity, their horizontal and vertical composability in deep architectures, their critical points, and their fundamental connections to clustering, Hebbian learning, and information theory.}
}
