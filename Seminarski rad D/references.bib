@InProceedings{pmlr-v27-baldi12a,
	title = 	 {Autoencoders, Unsupervised Learning, and Deep Architectures},
	author = 	 {Pierre Baldi},
	booktitle = 	 {Proceedings of ICML Workshop on Unsupervised and Transfer Learning},
	pages = 	 {37--49},
	year = 	 {2012},
	editor = 	 {Isabelle Guyon and Gideon Dror and Vincent Lemaire and Graham Taylor and Daniel Silver},
	volume = 	 {27},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Bellevue, Washington, USA},
	month = 	 {02 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf},
	url = 	 {http://proceedings.mlr.press/v27/baldi12a.html},
	abstract = 	 {Autoencoders play a fundamental role in unsupervised learning and in deep architectures for transfer learning and other tasks. In spite of their fundamental role, only linear autoencoders over the real numbers have been solved analytically. Here we present a general mathematical framework for the study of both linear and non-linear autoencoders. The framework allows one to derive an analytical treatment for the most non-linear autoencoder, the Boolean autoencoder. Learning in the Boolean autoencoder is equivalent to a clustering problem that can be solved in polynomial time when the number of clusters is small and becomes NP complete when the number of clusters is large. The framework sheds light on the different kinds of autoencoders, their learning complexity, their horizontal and vertical composability in deep architectures, their critical points, and their fundamental connections to clustering, Hebbian learning, and information theory.}
}

@article{lecun-mnisthandwrittendigit-2010,
	added-at = {2010-06-28T21:16:30.000+0200},
	author = {LeCun, Yann and Cortes, Corinna},
	biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
	groups = {public},
	howpublished = {http://yann.lecun.com/exdb/mnist/},
	interhash = {21b9d0558bd66279df9452562df6e6f3},
	intrahash = {935bad99fa1f65e03c25b315aa3c1032},
	keywords = {MSc _checked character_recognition mnist network neural},
	lastchecked = {2016-01-14 14:24:11},
	timestamp = {2016-07-12T19:25:30.000+0200},
	title = {{MNIST} handwritten digit database},
	url = {http://yann.lecun.com/exdb/mnist/},
	username = {mhwombat},
	year = 2010
}

@article{cifar10,
	title= {CIFAR-10 (Canadian Institute for Advanced Research)},
	journal= {},
	author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
	year= {},
	url= {http://www.cs.toronto.edu/~kriz/cifar.html},
	abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 
	
	The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
	keywords= {Dataset},
	terms= {}
}
